<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Accord.MachineLearning</name>
    </assembly>
    <members>
        <member name="T:Accord.MachineLearning.Bayes.NaiveBayes">
             <summary>
               Naïve Bayes Classifier.
             </summary>
             
             <remarks>
             <para>
               A naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem
               with strong (naive) independence assumptions. A more descriptive term for the underlying probability
               model would be "independent feature model".</para>
             <para>
               In simple terms, a naive Bayes classifier assumes that the presence (or absence) of a particular
               feature of a class is unrelated to the presence (or absence) of any other feature, given the class
               variable. In spite of their naive design and apparently over-simplified assumptions, naive Bayes 
               classifiers have worked quite well in many complex real-world situations.</para>
               
             <para>
               References:
               <list type="bullet">
                 <item><description>
                   Wikipedia contributors. "Naive Bayes classifier." Wikipedia, The Free Encyclopedia.
                   Wikipedia, The Free Encyclopedia, 16 Dec. 2011. Web. 5 Jan. 2012.</description></item>
               </list>
             </para>  
             </remarks>
             
             <example>
             <para>
               In this example, we will be using the famous Play Tennis example by Tom Mitchell (1998).
               In Mitchell's example, one would like to infer if a person would play tennis or not
               based solely on four input variables. Those variables are all categorical, meaning that
               there is no order between the possible values for the variable (i.e. there is no order
               relationship between Sunny and Rain, one is not bigger nor smaller than the other, but are 
               just distinct). Moreover, the rows, or instances presented below represent days on which the
               behavior of the person has been registered and annotated, pretty much building our set of 
               observation instances for learning:</para>
             
             <code>
               DataTable data = new DataTable("Mitchell's Tennis Example");
               
               data.Columns.Add("Day", "Outlook", "Temperature", "Humidity", "Wind", "PlayTennis");
               
               data.Rows.Add(   "D1",   "Sunny",      "Hot",       "High",   "Weak",    "No"  );
               data.Rows.Add(   "D2",   "Sunny",      "Hot",       "High",  "Strong",   "No"  ); 
               data.Rows.Add(   "D3",  "Overcast",    "Hot",       "High",   "Weak",    "Yes" );
               data.Rows.Add(   "D4",   "Rain",       "Mild",      "High",   "Weak",    "Yes" ); 
               data.Rows.Add(   "D5",   "Rain",       "Cool",     "Normal",  "Weak",    "Yes" ); 
               data.Rows.Add(   "D6",   "Rain",       "Cool",     "Normal", "Strong",   "No"  ); 
               data.Rows.Add(   "D7",  "Overcast",    "Cool",     "Normal", "Strong",   "Yes" );
               data.Rows.Add(   "D8",   "Sunny",      "Mild",      "High",   "Weak",    "No"  );  
               data.Rows.Add(   "D9",   "Sunny",      "Cool",     "Normal",  "Weak",    "Yes" ); 
               data.Rows.Add(   "D10", "Rain",        "Mild",     "Normal",  "Weak",    "Yes" ); 
               data.Rows.Add(   "D11",  "Sunny",      "Mild",     "Normal", "Strong",   "Yes" );
               data.Rows.Add(   "D12", "Overcast",    "Mild",      "High",  "Strong",   "Yes" ); 
               data.Rows.Add(   "D13", "Overcast",    "Hot",      "Normal",  "Weak",    "Yes" ); 
               data.Rows.Add(   "D14",  "Rain",       "Mild",      "High",  "Strong",   "No"  );
             </code>
             <para>
               <i>Obs: The DataTable representation is not required, and instead the NaiveBayes could
               also be trained directly on integer arrays containing the integer codewords.</i></para>
               
             <para>
               In order to estimate a discrete Naive Bayes, we will first convert this problem to a more simpler
               representation. Since all variables are categories, it does not matter if they are represented
               as strings, or numbers, since both are just symbols for the event they represent. Since numbers
               are more easily representable than text strings, we will convert the problem to use a discrete 
               alphabet through the use of a <see cref="T:Accord.Statistics.Filters.Codification">codebook</see>.</para>
             
             <para>
               A codebook effectively transforms any distinct possible value for a variable into an integer 
               symbol. For example, “Sunny” could as well be represented by the integer label 0, “Overcast” 
               by “1”, Rain by “2”, and the same goes by for the other variables. So:</para>
             
             <code>
               // Create a new codification codebook to 
               // convert strings into integer symbols
               Codification codebook = new Codification(data);
               
               // Translate our training data into integer symbols using our codebook:
               DataTable symbols = codebook.Apply(data); 
               int[][] inputs  = symbols.ToIntArray("Outlook", "Temperature", "Humidity", "Wind"); 
               int[]   outputs = symbols.ToIntArray("PlayTennis").GetColumn(0);
             </code>
             
             <para>
               Now that we already have our learning input/ouput pairs, we should specify our
               decision tree. We will be trying to build a tree to predict the last column, entitled
               “PlayTennis”. For this, we will be using the “Outlook”, “Temperature”, “Humidity” and
               “Wind” as predictors (variables which will we will use for our decision). Since those
               are categorical, we must specify, at the moment of creation of our tree, the
               number of each possible symbol for those variables.
             </para>
             
             <code>
               // Gather information about decision variables
               int[] symbolCounts =
               {
                 codebook["Outlook"].Symbols,     // 3 possible values (Sunny, overcast, rain)
                 codebook["Temperature"].Symbols, // 3 possible values (Hot, mild, cool)
                 codebook["Humidity"].Symbols,    // 2 possible values (High, normal)
                 codebook["Wind"].Symbols         // 2 possible values (Weak, strong)
               };
               
               int classCount = codebook["PlayTennis"].Symbols; // 2 possible values (yes, no)
            
               // Create a new Naive Bayes classifiers for the two classes
               NaiveBayes target = new NaiveBayes(classCount, symbolCounts);
               
               // Compute the Naive Bayes model
               target.Estimate(inputs, outputs);
             </code>
             
             <para>Now that we have created and estimated our classifier, we 
             can query the classifier for new input samples through the <see cref="M:Accord.MachineLearning.Bayes.NaiveBayes.Compute(System.Int32[])"/> method.</para>
             </example>
             
             <seealso cref="T:Accord.MachineLearning.Bayes.NaiveBayes`1"/>
             
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes.#ctor(System.Int32,System.Int32[])">
            <summary>
              Constructs a new Naïve Bayes Classifier.
            </summary>
            
            <param name="classes">The number of output classes.</param>
            <param name="symbols">The number of symbols for each input variable.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes.#ctor(System.Int32,System.Double[],System.Int32[])">
            <summary>
              Constructs a new Naïve Bayes Classifier.
            </summary>
            
            <param name="classes">The number of output classes.</param>
            <param name="classPriors">The prior probabilities for each output class.</param>
            <param name="symbols">The number of symbols for each input variable.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes.Estimate(System.Int32[][],System.Int32[],System.Boolean,System.Double)">
            <summary>
              Initializes the frequency tables of a Naïve Bayes Classifier.
            </summary>
            
            <param name="inputs">The input data.</param>
            <param name="outputs">The corresponding output labels for the input data.</param>
            <param name="empirical">True to estimate class priors from the data, false otherwise.</param>
            <param name="regularization">
              The amount of regularization to be used in the m-estimator. 
              Default is 1e-5.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes.Compute(System.Int32[])">
            <summary>
              Computes the most likely class for a given instance.
            </summary>
            
            <param name="input">The input instance.</param>
            <returns>The most likely class for the instance.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes.Compute(System.Int32[],System.Double@,System.Double[]@)">
            <summary>
              Computes the most likely class for a given instance.
            </summary>
            
            <param name="input">The input instance.</param>
            <param name="logLikelihood">The log-likelihood for the instance.</param>
            <param name="responses">The response probabilities for each class.</param>
            <returns>The most likely class for the instance.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes.ClassCount">
            <summary>
              Gets the number of possible output classes.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes.InputCount">
            <summary>
              Gets the number of inputs in the model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes.SymbolCount">
            <summary>
              Gets the number of symbols for each input in the model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes.Distributions">
            <summary>
              Gets the tables of log-probabilities for the frequence of
              occurance of each symbol for each class and input.
            </summary>
            
            <value>A double[,] array in with each row corresponds to a 
            class, each column corresponds to an input variable. Each
            element of this double[,] array is a frequency table containing
            the frequence of each symbol for the corresponding variable as
            a double[] array.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes.Priors">
            <summary>
              Gets the prior beliefs for each class.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.CrossValidation">
             <summary>
               k-Fold Cross-Validation.
             </summary>
             <remarks>
             <para>
               Cross-validation is a technique for estimating the performance of a predictive
               model. It can be used to measure how the results of a statistical analysis will
               generalize to an independent data set. It is mainly used in settings where the
               goal is prediction, and one wants to estimate how accurately a predictive model
               will perform in practice.</para>
             <para>
               One round of cross-validation involves partitioning a sample of data into
               complementary subsets, performing the analysis on one subset (called the
               training set), and validating the analysis on the other subset (called the
               validation set or testing set). To reduce variability, multiple rounds of 
               cross-validation are performed using different partitions, and the validation 
               results are averaged over the rounds.</para> 
               
             <para>
               References:
               <list type="bullet">
                 <item><description><a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)">
                   Wikipedia, The Free Encyclopedia. Cross-validation (statistics). Available on:
                   http://en.wikipedia.org/wiki/Cross-validation_(statistics) </a></description></item>
               </list></para> 
             </remarks>
             
             <example>
               <code>
               // This is a sample code on how to use Cross-Validation
               // to access the performance of Support Vector Machines.
            
               // Consider the example binary data. We will be trying
               // to learn a XOR problem and see how well does SVMs
               // perform on this data.
            
               double[][] data =
               {
                   new double[] { -1, -1 }, new double[] {  1, -1 },
                   new double[] { -1,  1 }, new double[] {  1,  1 },
                   new double[] { -1, -1 }, new double[] {  1, -1 },
                   new double[] { -1,  1 }, new double[] {  1,  1 },
                   new double[] { -1, -1 }, new double[] {  1, -1 },
                   new double[] { -1,  1 }, new double[] {  1,  1 },
                   new double[] { -1, -1 }, new double[] {  1, -1 },
                   new double[] { -1,  1 }, new double[] {  1,  1 },
               };
            
               int[] xor = // result of xor for the sample input data
               {
                   -1,       1,
                    1,      -1,
                   -1,       1,
                    1,      -1,
                   -1,       1,
                    1,      -1,
                   -1,       1,
                    1,      -1,
               };
            
            
               // Create a new Cross-validation algorithm passing the data set size and the number of folds
               var crossvalidation = new CrossValidation(size: data.Length, folds: 3);
            
               // Define a fitting function using Support Vector Machines. The objective of this
               // function is to learn a SVM in the subset of the data dicted by cross-validation.
            
               crossvalidation.Fitting = delegate(int k, int[] indicesTrain, int[] indicesValidation)
               {
                   // The fitting function is passing the indices of the original set which
                   // should be considered training data and the indices of the original set
                   // which should be considered validation data.
            
                   // Lets now grab the training data:
                   var trainingInputs = data.Submatrix(indicesTrain);
                   var trainingOutputs = xor.Submatrix(indicesTrain);
            
                   // And now the validation data:
                   var validationInputs = data.Submatrix(indicesValidation);
                   var validationOutputs = xor.Submatrix(indicesValidation);
            
            
                   // Create a Kernel Support Vector Machine to operate on the set
                   var svm = new KernelSupportVectorMachine(new Polynomial(2), 2);
            
                   // Create a training algorithm and learn the training data
                   var smo = new SequentialMinimalOptimization(svm, trainingInputs, trainingOutputs);
            
                   double trainingError = smo.Run();
            
                   // Now we can compute the validation error on the validation data:
                   double validationError = smo.ComputeError(validationInputs, validationOutputs);
            
                   // Return a new information structure containing the model and the errors achieved.
                   return new CrossValidationValues(svm, trainingError, validationError);
               };
            
            
               // Compute the cross-validation
               var result = crossvalidation.Compute();
            
               // Finally, access the measured performance.
               double trainingErrors = result.Training.Mean;
               double validationErrors = result.Validation.Mean;
               </code>
             </example>
             
        </member>
        <member name="T:Accord.MachineLearning.CrossValidation`1">
             <summary>
               k-Fold Cross-Validation.
             </summary>
             
             <typeparam name="TModel">The type of the model being analysed.</typeparam>
             
             <remarks>
             <para>
               Cross-validation is a technique for estimating the performance of a predictive
               model. It can be used to measure how the results of a statistical analysis will
               generalize to an independent data set. It is mainly used in settings where the
               goal is prediction, and one wants to estimate how accurately a predictive model
               will perform in practice.</para>
             <para>
               One round of cross-validation involves partitioning a sample of data into
               complementary subsets, performing the analysis on one subset (called the
               training set), and validating the analysis on the other subset (called the
               validation set or testing set). To reduce variability, multiple rounds of 
               cross-validation are performed using different partitions, and the validation 
               results are averaged over the rounds.</para> 
               
             <para>
               References:
               <list type="bullet">
                 <item><description><a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)">
                   Wikipedia, The Free Encyclopedia. Cross-validation (statistics). Available on:
                   http://en.wikipedia.org/wiki/Cross-validation_(statistics) </a></description></item>
               </list></para> 
             </remarks>
             
             <example>
               <code>
               // This is a sample code on how to use Cross-Validation
               // to access the performance of Support Vector Machines.
            
               // Consider the example binary data. We will be trying
               // to learn a XOR problem and see how well does SVMs
               // perform on this data.
            
               double[][] data =
               {
                   new double[] { -1, -1 }, new double[] {  1, -1 },
                   new double[] { -1,  1 }, new double[] {  1,  1 },
                   new double[] { -1, -1 }, new double[] {  1, -1 },
                   new double[] { -1,  1 }, new double[] {  1,  1 },
                   new double[] { -1, -1 }, new double[] {  1, -1 },
                   new double[] { -1,  1 }, new double[] {  1,  1 },
                   new double[] { -1, -1 }, new double[] {  1, -1 },
                   new double[] { -1,  1 }, new double[] {  1,  1 },
               };
            
               int[] xor = // result of xor for the sample input data
               {
                   -1,       1,
                    1,      -1,
                   -1,       1,
                    1,      -1,
                   -1,       1,
                    1,      -1,
                   -1,       1,
                    1,      -1,
               };
            
            
               // Create a new Cross-validation algorithm passing the data set size and the number of folds
               var crossvalidation = new CrossValidation&lt;KernelSupportVectorMachine>(size: data.Length, folds: 3);
            
               // Define a fitting function using Support Vector Machines. The objective of this
               // function is to learn a SVM in the subset of the data dicted by cross-validation.
            
               crossvalidation.Fitting = delegate(int k, int[] indicesTrain, int[] indicesValidation)
               {
                   // The fitting function is passing the indices of the original set which
                   // should be considered training data and the indices of the original set
                   // which should be considered validation data.
            
                   // Lets now grab the training data:
                   var trainingInputs = data.Submatrix(indicesTrain);
                   var trainingOutputs = xor.Submatrix(indicesTrain);
            
                   // And now the validation data:
                   var validationInputs = data.Submatrix(indicesValidation);
                   var validationOutputs = xor.Submatrix(indicesValidation);
            
            
                   // Create a Kernel Support Vector Machine to operate on the set
                   var svm = new KernelSupportVectorMachine(new Polynomial(2), 2);
            
                   // Create a training algorithm and learn the training data
                   var smo = new SequentialMinimalOptimization(svm, trainingInputs, trainingOutputs);
            
                   double trainingError = smo.Run();
            
                   // Now we can compute the validation error on the validation data:
                   double validationError = smo.ComputeError(validationInputs, validationOutputs);
            
                   // Return a new information structure containing the model and the errors achieved.
                   return new CrossValidationValues&lt;KernelSupportVectorMachine>(svm, trainingError, validationError);
               };
            
            
               // Compute the cross-validation
               var result = crossvalidation.Compute();
            
               // Finally, access the measured performance.
               double trainingErrors = result.Training.Mean;
               double validationErrors = result.Validation.Mean;
               </code>
             </example>
             
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation`1.#ctor(System.Int32)">
            <summary>
              Creates a new k-fold cross-validation algorithm.
            </summary>
            
            <param name="size">The total number of available samples.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation`1.#ctor(System.Int32,System.Int32)">
            <summary>
              Creates a new k-fold cross-validation algorithm.
            </summary>
            
            <param name="size">The total number of available samples.</param>
            <param name="folds">The number of folds, usually denoted as <c>k</c> (default is 10).</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation`1.#ctor(System.Int32[],System.Int32)">
            <summary>
              Creates a new k-fold cross-validation algorithm.
            </summary>
            
            <param name="indices">An already created set of fold indices for each sample in a dataset.</param>
            <param name="folds">The total number of folds referenced in the <paramref name="indices"/> param.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation`1.CreatePartitions(System.Int32,System.Int32[]@,System.Int32[]@)">
            <summary>
              Gets the indices for the training and validation 
              sets for the specified validation fold index.
            </summary>
            
            <param name="validationFoldIndex">The index of the validation fold.</param>
            <param name="trainingSet">The indices for the observations in the training set.</param>
            <param name="validationSet">The indices for the observations in the validation set.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation`1.GetPartitionSize(System.Int32,System.Int32@,System.Int32@)">
            <summary>
              Gets the number of instances in training and validation
              sets for the specified validation fold index.
            </summary>
            
            <param name="validationFoldIndex">The index of the validation fold.</param>
            <param name="trainingCount">The number of instances in the training set.</param>
            <param name="validationCount">The number of instances in the validation set.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation`1.Compute">
            <summary>
              Computes the cross validation algorithm.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidation`1.Fitting">
            <summary>
              Gets or sets the model fitting function.
            </summary>
            <remarks>
              The fitting function should accept an array of integers containing the
              indexes for the training samples, an array of integers containing the
              indexes for the validation samples and should return information about
              the model fitted using those two subsets of the available data.
            </remarks>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidation`1.Folds">
            <summary>
              Gets the array of data set indexes contained in each fold.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidation`1.Indices">
            <summary>
             Gets the array of fold indices for each point in the data set.
            </summary>
        </member>
        <member name="P:Accord.MachineLearning.CrossValidation`1.K">
            <summary>
              Gets the number of folds in the k-fold cross validation.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidation`1.Samples">
            <summary>
              Gets the total number of data samples in the data set.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidation`1.RunInParallel">
            <summary>
              Gets or sets a value indicating whether to use parallel
              processing through the use of multiple threads or not.
              Default is true.
            </summary>
            
            <value><c>true</c> to use multiple threads; otherwise, <c>false</c>.</value>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation.#ctor(System.Int32)">
            <summary>
              Creates a new k-fold cross-validation algorithm.
            </summary>
            
            <param name="size">The total number of available samples.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation.#ctor(System.Int32,System.Int32)">
            <summary>
              Creates a new k-fold cross-validation algorithm.
            </summary>
            
            <param name="size">The complete dataset for training and testing.</param>
            <param name="folds">The number of folds, usually denoted as <c>k</c> (default is 10).</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation.#ctor(System.Int32[],System.Int32)">
            <summary>
              Creates a new k-fold cross-validation algorithm.
            </summary>
            
            <param name="indices">An already created set of fold indices for each sample in a dataset.</param>
            <param name="folds">The total number of folds referenced in the <paramref name="indices"/> param.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidation.Splittings(System.Int32,System.Int32)">
            <summary>
              Create cross-validation folds by generating
              a vector of random fold indices.
            </summary>
            
            <param name="size">The number of points in the data set.</param>
            <param name="folds">The number of folds in the cross-validation.</param>
            
            <returns>A vector of indices defining the a fold for each point in the data set.</returns>
            
        </member>
        <member name="T:Accord.MachineLearning.CrossValidationStatistics">
            <summary>
              Summary statistics for a Cross-validation trial.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationStatistics.#ctor(System.Int32[],System.Double[])">
            <summary>
              Create a new cross-validation statistics class.
            </summary>
            
            <param name="sizes">The number of samples used to compute the statistics.</param>
            <param name="statistics">The performance statistics gathered during the run.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationStatistics.#ctor(System.Int32[],System.Double[],System.Double[])">
            <summary>
              Create a new cross-validation statistics class.
            </summary>
            
            <param name="sizes">The number of samples used to compute the statistics.</param>
            <param name="statistics">The performance statistics gathered during the run.</param>
            <param name="variances">The variance of the statistics gathered during the run, if available.</param>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationStatistics.Values">
            <summary>
              Gets the values acquired during the cross-validation.
              Most often those will be the errors for each folding.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationStatistics.Variances">
            <summary>
              Gets the variance for each value acquired during the cross-validation.
              Most often those will be the error variance for each folding.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationStatistics.Sizes">
            <summary>
              Gets the number of samples used to compute the variance
              of the values acquired during the cross-validation.
            </summary>
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationStatistics.Mean">
            <summary>
              Gets the mean of the performance statistics.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationStatistics.Variance">
            <summary>
              Gets the variance of the performance statistics.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationStatistics.StandardDeviation">
            <summary>
              Gets the standard deviation of the performance statistics.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationStatistics.PooledVariance">
            <summary>
              Gets the pooled variance of the performance statistics.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationStatistics.PooledStandardDeviation">
            <summary>
              Gets the pooled standard deviation of the performance statistics.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.BootstrapFittingFunction">
            <summary>
              Fitting function delegate.
            </summary>
            
            <param name="trainingSamples">
              The sample indexes to be used as training samples in
              the model fitting procedure. </param>
            <param name="validationSamples">
              The sample indexes to be used as validation samples in
              the model fitting procedure. </param>
              
            <remarks>
              The fitting function is called during the Bootstrap
              procedure to fit a model with the given set of samples
              for training and validation.
            </remarks>
            
        </member>
        <member name="T:Accord.MachineLearning.Bootstrap">
            <summary>
              Boostrap method for generalization
              performance measurements.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.Bootstrap.#ctor(System.Int32,System.Int32)">
            <summary>
              Creates a new Bootstrap estimation algorithm.
            </summary>
            
            <param name="dataSize">The size of the complete dataset.</param>
            <param name="subsamples">The number B of bootstrap resamplings to perform.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bootstrap.#ctor(System.Int32,System.Int32,System.Int32)">
            <summary>
              Creates a new Bootstrap estimation algorithm.
            </summary>
            
            <param name="dataSize">The size of the complete dataset.</param>
            <param name="subsamples">The number B of bootstrap resamplings to perform.</param>
            <param name="subsampleSize">The number of samples in each subsample. Default
              is to use the total number of samples in the population dataset.</param>.
            
        </member>
        <member name="M:Accord.MachineLearning.Bootstrap.#ctor(System.Int32,System.Int32[][])">
            <summary>
              Creates a new Bootstrap estimation algorithm.
            </summary>
            
            <param name="dataSize">The size of the complete dataset.</param>
            <param name="resamplings">The indices of the bootstrap samplings.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bootstrap.CreatePartitions(System.Int32,System.Int32[]@,System.Int32[]@)">
            <summary>
              Gets the indices for the training and validation 
              sets for the specified validation fold index.
            </summary>
            
            <param name="index">The index of the validation fold.</param>
            <param name="trainingSet">The indices for the observations in the training set.</param>
            <param name="validationSet">The indices for the observations in the validation set.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bootstrap.Compute">
            <summary>
              Computes the cross validation algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.Bootstrap.GetPartitionSize(System.Int32,System.Int32@,System.Int32@)">
            <summary>
              Gets the number of instances in training and validation
              sets for the specified validation fold index.
            </summary>
            
            <param name="index">The index of the bootstrap sample.</param>
            <param name="trainingCount">The number of instances in the training set.</param>
            <param name="validationCount">The number of instances in the validation set.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bootstrap.Samplings(System.Int32,System.Int32,System.Int32)">
            <summary>
              Draws the bootstrap samples from the population.
            </summary>
            
            <param name="size">The size of the samples to be drawn.</param>
            <param name="resamplings">The number of samples to drawn.</param>
            <param name="subsampleSize">The size of the samples to be drawn.</param>
            
            <returns>The indices of the samples in the original set.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.Bootstrap.B">
            <summary>
              Gets the number B of bootstrap samplings
              to be drawn from the population dataset.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.Bootstrap.Samples">
            <summary>
              Gets the total number of samples in the population dataset.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.Bootstrap.Subsamples">
            <summary>
              Gets the bootstrap samples drawn from
              the population dataset as indices.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.Bootstrap.Fitting">
            <summary>
              Gets or sets the model fitting function.
            </summary>
            <remarks>
              The fitting function should accept an array of integers containing the
              indexes for the training samples, an array of integers containing the
              indexes for the validation samples and should return information about
              the model fitted using those two subsets of the available data.
            </remarks>
            
        </member>
        <member name="P:Accord.MachineLearning.Bootstrap.RunInParallel">
            <summary>
              Gets or sets a value indicating whether to use parallel
              processing through the use of multiple threads or not.
              Default is true.
            </summary>
            
            <value><c>true</c> to use multiple threads; otherwise, <c>false</c>.</value>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.SupportVectorMachine">
            <summary>
              Sparse Linear Support Vector Machine (SVM)
            </summary>
            
            <remarks>
            <para>
              Support vector machines (SVMs) are a set of related supervised learning methods
              used for classification and regression. In simple words, given a set of training
              examples, each marked as belonging to one of two categories, a SVM training algorithm
              builds a model that predicts whether a new example falls into one category or the
              other.</para>
            <para>
              Intuitively, an SVM model is a representation of the examples as points in space,
              mapped so that the examples of the separate categories are divided by a clear gap
              that is as wide as possible. New examples are then mapped into that same space and
              predicted to belong to a category based on which side of the gap they fall on.</para>
              
            <para>
              References:
              <list type="bullet">
                <item><description><a href="http://en.wikipedia.org/wiki/Support_vector_machine">
                  http://en.wikipedia.org/wiki/Support_vector_machine </a></description></item>
              </list></para>   
            </remarks>
            
            <example>
              <code>
              // Example AND problem
              double[][] inputs =
              {
                  new double[] { 0, 0 }, // 0 and 0: 0 (label -1)
                  new double[] { 0, 1 }, // 0 and 1: 0 (label -1)
                  new double[] { 1, 0 }, // 1 and 0: 0 (label -1)
                  new double[] { 1, 1 }  // 1 and 1: 1 (label +1)
              };
              
              // Dichotomy SVM outputs should be given as [-1;+1]
              int[] labels =
              {
                  // 0,  0,  0, 1
                    -1, -1, -1, 1
              };
              
              // Create a Support Vector Machine for the given inputs
              SupportVectorMachine machine = new SupportVectorMachine(inputs[0].Length);
              
              // Instantiate a new learning algorithm for SVMs
              SequentialMinimalOptimization smo = new SequentialMinimalOptimization(machine, inputs, labels);
              
              // Set up the learning algorithm
              smo.Complexity = 1.0;
              
              // Run the learning algorithm
              double error = smo.Run();
            
              // Compute the decision output for one of the input vectors
              int decision = System.Math.Sign(svm.Compute(inputs[0]));
              </code>
            </example>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.ISupportVectorMachine">
            <summary>
              Common interface for Support Vector Machines
            </summary>
            
            <seealso cref="T:Accord.MachineLearning.VectorMachines.SupportVectorMachine"/>
            <seealso cref="T:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine"/>
            <seealso cref="T:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine"/>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.ISupportVectorMachine.Compute(System.Double[],System.Double@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <param name="inputs">An input vector.</param>
            
            <param name="output">The output for the given input.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.SupportVectorMachine.#ctor(System.Int32)">
            <summary>
              Creates a new Support Vector Machine
            </summary>
            
            <param name="inputs">The number of inputs for the machine.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Compute(System.Double[],System.Double@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <remarks>
              For a binary decision problem, the decision for the negative
              or positive class is typically computed by taking the sign of
              the machine's output.
            </remarks>
            
            <param name="inputs">An input vector.</param>
            <param name="output">The output of the machine. If this is a 
              <see cref="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.IsProbabilistic">probabilistic</see> machine, the
              output is the probability of the positive class. If this is
              a standard machine, the output is the distance to the decision
              hyperplane in feature space.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Compute(System.Double[])">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <remarks>
              For a binary decision problem, the decision for the negative
              or positive class is typically computed by taking the sign of
              the machine's output.
            </remarks>
            
            <param name="inputs">An input vector.</param>
            
            <returns>The decision label for the given input.</returns>
             
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Save(System.IO.Stream)">
            <summary>
              Saves the machine to a stream.
            </summary>
            
            <param name="stream">The stream to which the machine is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Save(System.String)">
            <summary>
              Saves the machine to a stream.
            </summary>
            
            <param name="path">The stream to which the machine is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Load(System.IO.Stream)">
            <summary>
              Loads a machine from a stream.
            </summary>
            
            <param name="stream">The stream from which the machine is to be deserialized.</param>
            
            <returns>The deserialized machine.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Load(System.String)">
            <summary>
              Loads a machine from a file.
            </summary>
            
            <param name="path">The path to the file from which the machine is to be deserialized.</param>
            
            <returns>The deserialized machine.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Link">
            <summary>
              Gets or sets the <see cref="T:Accord.Statistics.Links.ILinkFunction">link
              function</see> used by this machine, if any.
            </summary>
            
            <value>The link function used to transform machine outputs.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.IsProbabilistic">
            <summary>
              Gets a value indicating whether this machine produces probabilistic outputs.
            </summary>
            
            <value>
              <c>true</c> if this machine produces probabilistic outputs; otherwise, <c>false</c>.
            </value>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Inputs">
            <summary>
              Gets the number of inputs accepted by this machine.
            </summary>
            
            <remarks>
              If the number of inputs is zero, this means the machine
              accepts a indefinite number of inputs. This is often the
              case for kernel vector machines using a sequence kernel.
            </remarks>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.SupportVectors">
            <summary>
              Gets or sets the collection of support vectors used by this machine.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.IsCompact">
            <summary>
              Gets whether this machine is in compact mode. Compact
              machines do not need to keep storing their support vectors.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Weights">
            <summary>
              Gets or sets the collection of weights used by this machine.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.Threshold">
            <summary>
              Gets or sets the threshold (bias) term for this machine.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.SupportVectorReduction">
            <summary>
              Exact support vector reduction through 
              linear dependency elimination.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.ISupportVectorMachineLearning">
            <summary>
              Common interface for Support Machine Vector learning algorithms.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.ISupportVectorMachineLearning.Run">
            <summary>
              Runs the learning algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.ISupportVectorMachineLearning.Run(System.Boolean)">
            <summary>
              Runs the learning algorithm.
            </summary>
            
            <param name="computeError">
              True to compute error after the training
              process completes, false otherwise.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SupportVectorReduction.#ctor(Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine)">
            <summary>
              Creates a new <see cref="T:Accord.MachineLearning.VectorMachines.Learning.SupportVectorReduction"/> algorithm.
            </summary>
            
            <param name="machine">The machine to be reducted.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SupportVectorReduction.Run(System.Boolean)">
            <summary>
              Runs the learning algorithm.
            </summary>
            
            <param name="computeError">True to compute error after the training
            process completes, false otherwise.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SupportVectorReduction.Run">
            <summary>
              Runs the learning algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SupportVectorReduction.ComputeError(System.Double[][],System.Int32[])">
            <summary>
              Computes the error rate for a given set of input and outputs.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.CrossValidationFittingFunction`1">
            <summary>
              Fitting function delegate.
            </summary>
            
            <param name="k">
              The fold index.</param>
            <param name="trainingSamples">
              The sample indexes to be used as training samples in
              the model fitting procedure. </param>
            <param name="validationSamples">
              The sample indexes to be used as validation samples in
              the model fitting procedure. </param>
              
            <remarks>
              The fitting function is called during the Cross-validation
              procedure to fit a model with the given set of samples for
              training and validation.
            </remarks>
            
        </member>
        <member name="T:Accord.MachineLearning.CrossValidationResult`1">
            <summary>
              Class for representing results acquired through
              a k-fold cross-validation analysis.
            </summary>
            
            <typeparam name="TModel">The type of the model being analysed.</typeparam>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationResult`1.#ctor(Accord.MachineLearning.CrossValidation{`0},Accord.MachineLearning.CrossValidationValues{`0}[])">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.MachineLearning.CrossValidationResult`1"/> class.
            </summary>
            
            <param name="owner">The <see cref="T:Accord.MachineLearning.CrossValidation`1"/> that is creating this result.</param>
            <param name="models">The models created during the cross-validation runs.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationResult`1.Save(System.IO.Stream)">
            <summary>
              Saves the result to a stream.
            </summary>
            
            <param name="stream">The stream to which the result is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationResult`1.Save(System.String)">
            <summary>
              Saves the result to a stream.
            </summary>
            
            <param name="path">The stream to which the result is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationResult`1.Load(System.IO.Stream)">
            <summary>
              Loads a result from a stream.
            </summary>
            
            <param name="stream">The stream from which the result is to be deserialized.</param>
            
            <returns>The deserialized result.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationResult`1.Load(System.String)">
            <summary>
              Loads a result from a stream.
            </summary>
            
            <param name="path">The path to the file from which the result is to be deserialized.</param>
            
            <returns>The deserialized result.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationResult`1.Settings">
            <summary>
              Gets the <see cref="T:Accord.MachineLearning.CrossValidation`1"/>   
              object used to generate this result.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationResult`1.Training">
            <summary>
              Gets the performance statistics for the training set.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationResult`1.Validation">
            <summary>
              Gets the performance statistics for the validation set.
            </summary>
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationResult`1.Models">
            <summary>
              Gets the models created for each fold of the cross validation.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.GridSearchRange">
            <summary>
              Represents a range of parameters to be tried during a grid search.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchRange.#ctor(System.String,System.Double,System.Double,System.Double)">
            <summary>
              Constructs a new GridsearchRange object.
            </summary>
            
            <param name="name">The name for this parameter.</param>
            <param name="start">The starting value for this range.</param>
            <param name="end">The end value for this range.</param>
            <param name="step">The step size for this range.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchRange.#ctor(System.String,System.Double[])">
            <summary>
              Constructs a new GridSearchRange object.
            </summary>
            
            <param name="name">The name for this parameter.</param>
            <param name="values">The array of values to try.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchRange.GetParameters">
            <summary>
              Gets the array of GridSearchParameters to try.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GridSearchRange.Name">
            <summary>
              Gets or sets the name of the parameter from which the range belongs to.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GridSearchRange.Values">
            <summary>
              Gets or sets the range of values that should be tested for this parameter.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.GridSearchRangeCollection">
            <summary>
              GridSearchRange collection.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchRangeCollection.#ctor(Accord.MachineLearning.GridSearchRange[])">
            <summary>
              Constructs a new collection of GridsearchRange objects.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchRangeCollection.GetKeyForItem(Accord.MachineLearning.GridSearchRange)">
            <summary>
              Returns the identifying value for an item on this collection.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchRangeCollection.Add(System.String,System.Double[])">
            <summary>
              Adds a parameter range to the end of the GridsearchRangeCollection.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.ModelStorageMode">
            <summary>
              Modes for storing models.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.ModelStorageMode.AllModels">
            <summary>
              Stores a model on each iteration. This is the most
              intensive method, but enables a quick restoration 
              of any point on the learning history.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.ModelStorageMode.MinimumOnly">
            <summary>
              Stores only the model which had shown the minimum 
              validation value in the training history. All other
              models are discarded and only their validation and
              training values will be registered.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.ModelStorageMode.MaximumOnly">
            <summary>
              Stores only the model which had shown the maximum 
              validation value in the training history. All other
              models are discarded and only their validation and
              training values will be registered.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.EarlyStopping`1">
            <summary>
              Early stopping training procedure.
            </summary>
            
            <remarks>
              The early stopping training procedure monitorates a validation set
              during training to determine when a learning algorithm has stopped
              learning and started to overfit data. This class keeps an history
              of training and validation errors and will keep the best model found
              during learning.
            </remarks>
            
            <typeparam name="TModel">The type of the model to be trained.</typeparam>
            
        </member>
        <member name="M:Accord.MachineLearning.EarlyStopping`1.#ctor">
            <summary>
              Creates a new early stopping procedure object.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.EarlyStopping`1.Compute">
            <summary>
              Starts the model training, calling the <see cref="P:Accord.MachineLearning.EarlyStopping`1.IterationFunction"/>
              on each iteration.
            </summary>
            
            <returns>True if the model training has converged, false otherwise.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.EarlyStopping`1.MaxIterations">
            <summary>
              Gets or sets the maximum number of iterations
              performed by the early stopping algorithm. Default
              is 0 (run until convergence).
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.EarlyStopping`1.Tolerance">
            <summary>
              Gets or sets the minimum tolerance value used
              to determine convergence. Default is 1e-5.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.EarlyStopping`1.History">
            <summary>
              Gets the history of training and validation values 
              registered at each iteration of the learning algorithm.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.EarlyStopping`1.MinValidationValue">
            <summary>
              Gets the model with minimum validation error found during learning.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.EarlyStopping`1.MaxValidationValue">
            <summary>
              Gets the model with maximum validation error found during learning.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.EarlyStopping`1.Mode">
            <summary>
              Gets or sets the storage policy for the procedure.
            </summary>
        </member>
        <member name="P:Accord.MachineLearning.EarlyStopping`1.IterationFunction">
            <summary>
              Gets or sets the iteration function for the procedure. This
              function will be called on each iteration and should run one
              iteration of the learning algorithm for the given model.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.BootstrapResult">
            <summary>
              Class for representing results acquired
              through a bootstrap validation analysis.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.BootstrapResult.#ctor(Accord.MachineLearning.Bootstrap,Accord.MachineLearning.BootstrapValues[])">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.MachineLearning.BootstrapResult"/> class.
            </summary>
            
            <param name="owner">The <see cref="T:Accord.MachineLearning.Bootstrap"/> that is creating this result.</param>
            <param name="models">The models created during the cross-validation runs.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.BootstrapResult.Save(System.IO.Stream)">
            <summary>
              Saves the result to a stream.
            </summary>
            
            <param name="stream">The stream to which the result is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.BootstrapResult.Save(System.String)">
            <summary>
              Saves the result to a stream.
            </summary>
            
            <param name="path">The stream to which the result is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.BootstrapResult.Load(System.IO.Stream)">
            <summary>
              Loads a result from a stream.
            </summary>
            
            <param name="stream">The stream from which the result is to be deserialized.</param>
            
            <returns>The deserialized result.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.BootstrapResult.Load(System.String)">
            <summary>
              Loads a result from a stream.
            </summary>
            
            <param name="path">The path to the file from which the result is to be deserialized.</param>
            
            <returns>The deserialized result.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.BootstrapResult.Settings">
            <summary>
              Gets the <see cref="T:Accord.MachineLearning.Bootstrap"/>   
              object used to generate this result.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.BootstrapResult.Training">
            <summary>
              Gets the performance statistics for the training set.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.BootstrapResult.Validation">
            <summary>
              Gets the performance statistics for the validation set.
            </summary>
        </member>
        <member name="P:Accord.MachineLearning.BootstrapResult.Estimate">
            <summary>
              Gets the 0.632 bootstrap estimate.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KNearestNeighbor">
            <summary>
              K-Nearest Neighbor (k-NN) algorithm.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KNearestNeighbor`1">
            <summary>
              K-Nearest Neighbor (k-NN) algorithm.
            </summary>
            
            <typeparam name="T">The type of the input data.</typeparam>
            
        </member>
        <member name="M:Accord.MachineLearning.KNearestNeighbor`1.#ctor(System.Int32,`0[][],System.Int32[],System.Func{`0[],`0[],System.Double})">
            <summary>
              Creates a new <see cref="T:Accord.MachineLearning.KNearestNeighbor"/>.
            </summary>
            
            <param name="k">The number of nearest neighbors to be used in the decision.</param>
            
            <param name="inputs">The input data points.</param>
            <param name="outputs">The associated labels for the input points.</param>
            <param name="distance">The distance measure to use in the decision.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.KNearestNeighbor`1.Compute(`0[])">
            <summary>
              Computes the most likely label of a new given point.
            </summary>
            
            <param name="input">A point to be classificated.</param>
            
            <returns>The most likely label for the given point.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.KNearestNeighbor`1.Inputs">
            <summary>
              Gets the set of points given
              as input of the algorithm.
            </summary>
            
            <value>The input points.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.KNearestNeighbor`1.Outputs">
            <summary>
              Gets the set of labels associated
              with each <see cref="P:Accord.MachineLearning.KNearestNeighbor`1.Inputs"/> point.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KNearestNeighbor`1.Distance">
            <summary>
              Gets or sets the distance function used
              as a distance metric between data points.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KNearestNeighbor`1.K">
            <summary>
              Gets or sets the number of nearest 
              neighbors to be used in the decision.
            </summary>
            
            <value>The number of neighbors.</value>
            
        </member>
        <member name="M:Accord.MachineLearning.KNearestNeighbor.#ctor(System.Int32,System.Double[][],System.Int32[])">
            <summary>
              Creates a new <see cref="T:Accord.MachineLearning.KNearestNeighbor"/>.
            </summary>
            
            <param name="k">The number of nearest neighbors to be used in the decision.</param>
            
            <param name="inputs">The input data points.</param>
            <param name="outputs">The associated labels for the input points.</param>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.DecisionTreeExpressionCreator">
            <summary>
              Decision Tree (Linq) Expression Creator.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionTreeExpressionCreator.#ctor(Accord.MachineLearning.DecisionTrees.DecisionTree)">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionTreeExpressionCreator"/> class.
            </summary>
            
            <param name="tree">The decision tree.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionTreeExpressionCreator.Create">
            <summary>
              Creates an expression for the tree.
            </summary>
            
            <returns>A strongly typed lambda expression in the form
            of an <see cref="T:System.Linq.Expressions.Expression`1">expression</see> tree
            representing the <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionTree"/>.</returns>
            
        </member>
        <member name="T:Accord.MachineLearning.BootstrapValues">
            <summary>
              Information class to store the training and validation errors of a model. 
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.BootstrapValues.#ctor(System.Double,System.Double)">
            <param name="trainingValue">The training value for the model.</param>
            <param name="validationValue">The validation value for the model.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.BootstrapValues.#ctor(System.Double,System.Double,System.Double,System.Double)">
            <param name="trainingValue">The training value for the model.</param>
            <param name="validationValue">The validation value for the model.</param>
            <param name="trainingVariance">The variance of the training values.</param>
            <param name="validationVariance">The variance of the validation values.</param>
            
        </member>
        <member name="P:Accord.MachineLearning.BootstrapValues.ValidationValue">
            <summary>
              Gets the validation value for the model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.BootstrapValues.ValidationVariance">
            <summary>
              Gets the variance of the validation 
              value for the model, if available.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.BootstrapValues.TrainingValue">
            <summary>
              Gets the training value for the model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.BootstrapValues.TrainingVariance">
            <summary>
              Gets the variance of the training 
              value for the model, if available.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.BootstrapValues.Tag">
            <summary>
              Gets or sets a tag for user-defined information.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.ProbabilisticOutputLearning">
            <summary>
              Probabilistic Output Calibration.
            </summary>
            
            <remarks>
              <para>Instead of producing probabilistic outputs, Support Vector Machines
              express their decisions in the form of a distance from support vectors in
              feature space. In order to convert the SVM outputs into probabilities,
              Platt (1999) proposed the calibration of the SVM outputs using a sigmoid
              (logit) link function. Later, Lin et al (2007) provided a corrected and
              improved version of Platt's probabilistic outputs. This clas implements
              the later.</para>
              
              <para>This class is not an actual learning algorithm, but a calibrator.
              Machines passed as input to this algorithm should already have been trained
              by a proper learning algorithm such as <see cref="T:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization">
              Sequential Minimal Optimization (SMO)</see>.</para>
              
            <para>
              References:
              <list type="bullet">
                <item><description>
                   John C. Platt. 1999. Probabilistic Outputs for Support Vector Machines and Comparisons to
                   Regularized Likelihood Methods. In ADVANCES IN LARGE MARGIN CLASSIFIERS (1999), pp. 61-74.</description></item>
                <item><description>
                  Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007. A note on Platt's probabilistic outputs
                  for support vector machines. Mach. Learn. 68, 3 (October 2007), 267-276. </description></item>
              </list></para>   
            </remarks>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.ProbabilisticOutputLearning.#ctor(Accord.MachineLearning.VectorMachines.SupportVectorMachine,System.Double[][],System.Int32[])">
            <summary>
              Initializes a new instance of Platt's Probabilistic Output Calibration algorithm.
            </summary>
            
            <param name="machine">A Support Vector Machine.</param>
            <param name="inputs">The input data points as row vectors.</param>
            <param name="outputs">The classification label for each data point in the range [-1;+1].</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.ProbabilisticOutputLearning.Run">
            <summary>
              Runs the calibration algorithm.
            </summary>
            
            <returns>
              The log-likelihood of the calibrated model.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.ProbabilisticOutputLearning.Run(System.Boolean)">
            <summary>
              Runs the calibration algorithm.
            </summary>
            
            <param name="computeError">
              True to compute error after the training
              process completes, false otherwise. Default is true.
            </param>
            
            <returns>
              The log-likelihood of the calibrated model.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.ProbabilisticOutputLearning.LogLikelihood(System.Double[][],System.Int32[])">
            <summary>
              Computes the log-likelihood of the current model
              for the given inputs and outputs.
            </summary>
            
            <param name="inputs">The input data.</param>
            <param name="outputs">The corresponding outputs.</param>
            
            <returns>The log-likelihood of the model.</returns>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.Learning.Measures">
            <summary>
              Static class for common information measures.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.Learning.Measures.SplitInformation(System.Int32,System.Int32[][])">
            <summary>
              Computes the split information measure.
            </summary>
            
            <param name="samples">The total number of samples.</param>
            <param name="partitions">The partitioning.</param>
            <returns>The split information for the given partitions.</returns>
            
        </member>
        <member name="T:Accord.MachineLearning.RANSAC`1">
            <summary>
              Multipurpose RANSAC algorithm.
            </summary>
            
            <typeparam name="TModel">The model type to be trained by RANSAC.</typeparam>
            
            <remarks>
            <para>
              RANSAC is an abbreviation for "RANdom SAmple Consensus". It is an iterative
              method to estimate parameters of a mathematical model from a set of observed
              data which contains outliers. It is a non-deterministic algorithm in the sense
              that it produces a reasonable result only with a certain probability, with this
              probability increasing as more iterations are allowed.</para>
              
            <para>
              References:
              <list type="bullet">
                <item><description>
                  P. D. Kovesi. MATLAB and Octave Functions for Computer Vision and Image Processing.
                  School of Computer Science and Software Engineering, The University of Western Australia.
                  Available in: http://www.csse.uwa.edu.au/~pk/research/matlabfns </description></item>
                <item><description>
                  Wikipedia, The Free Encyclopedia. RANSAC. Available on:
                  http://en.wikipedia.org/wiki/RANSAC </description></item>
              </list>
            </para>
            </remarks>
            
        </member>
        <member name="M:Accord.MachineLearning.RANSAC`1.#ctor(System.Int32)">
            <summary>
              Constructs a new RANSAC algorithm.
            </summary>
            
            <param name="minSamples">
              The minimum number of samples from the data
              required by the fitting function to fit a model.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.RANSAC`1.#ctor(System.Int32,System.Double)">
            <summary>
              Constructs a new RANSAC algorithm.
            </summary>
            
            <param name="minSamples">
              The minimum number of samples from the data
              required by the fitting function to fit a model.
            </param>
            <param name="threshold">
              The minimum distance between a data point and
              the model used to decide whether the point is
              an inlier or not.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.RANSAC`1.#ctor(System.Int32,System.Double,System.Double)">
            <summary>
              Constructs a new RANSAC algorithm.
            </summary>
            
            <param name="minSamples">
              The minimum number of samples from the data
              required by the fitting function to fit a model.
            </param>
            <param name="threshold">
              The minimum distance between a data point and
              the model used to decide whether the point is
              an inlier or not.
            </param>
            <param name="probability">
              The probability of obtaining a random sample of
              the input points that contains no outliers.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.RANSAC`1.Compute(System.Int32)">
            <summary>
              Computes the model using the RANSAC algorithm.
            </summary>
            
            <param name="size">The total number of points in the data set.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.RANSAC`1.Compute(System.Int32,System.Int32[]@)">
            <summary>
              Computes the model using the RANSAC algorithm.
            </summary>
            
            <param name="size">The total number of points in the data set.</param>
            <param name="inliers">The indexes of the outlier points in the data set.</param>
            
        </member>
        <member name="P:Accord.MachineLearning.RANSAC`1.Fitting">
            <summary>
              Model fitting function.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.RANSAC`1.Degenerate">
            <summary>
              Degenerative set detection function.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.RANSAC`1.Distances">
            <summary>
              Distance function.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.RANSAC`1.Threshold">
            <summary>
              Gets or sets the minimum distance between a data point and
              the model used to decide whether the point is an inlier or not.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.RANSAC`1.Samples">
            <summary>
              Gets or sets the minimum number of samples from the data
              required by the fitting function to fit a model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.RANSAC`1.MaxSamplings">
            <summary>
              Maximum number of attempts to select a non-degenerate data set.
            </summary>
            
            <remarks>
              The default value is 100.
            </remarks>
            
        </member>
        <member name="P:Accord.MachineLearning.RANSAC`1.MaxEvaluations">
            <summary>
              Maximum number of iterations.
            </summary>
            
            <remarks>
              The default value is 1000.
            </remarks>
            
        </member>
        <member name="P:Accord.MachineLearning.RANSAC`1.Probability">
            <summary>
              Gets or sets the probability of obtaining a random
              sample of the input points that contains no outliers.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.Bayes.NaiveBayes`1">
             <summary>
               Naïve Bayes Classifier for arbitrary distributions.
             </summary>
             
             <remarks>
             <para>
               A naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem
               with strong (naive) independence assumptions. A more descriptive term for the underlying probability
               model would be "independent feature model".</para>
             <para>
               In simple terms, a naive Bayes classifier assumes that the presence (or absence) of a particular
               feature of a class is unrelated to the presence (or absence) of any other feature, given the class
               variable. In spite of their naive design and apparently over-simplified assumptions, naive Bayes 
               classifiers have worked quite well in many complex real-world situations.</para>
               
             <para>
               References:
               <list type="bullet">
                 <item><description>
                   Wikipedia contributors. "Naive Bayes classifier." Wikipedia, The Free Encyclopedia.
                   Wikipedia, The Free Encyclopedia, 16 Dec. 2011. Web. 5 Jan. 2012.</description></item>
               </list>
             </para>  
             </remarks>
             
             <example>
             <para>
               In this example, we will be using a mixed-continuous version of the famous Play Tennis
               example by Tom Mitchell (1998). In Mitchell's example, one would like to infer if a person
               would play tennis or not based solely on four input variables. The original variables were
               categorical, but in this example, two of them will be categorical and two will be continuous.
               Rhe rows, or instances presented below represent days on which the behavior of the person
               has been registered and annotated, pretty much building our set of observation instances for
               learning:</para>
             
             <code>
               DataTable data = new DataTable("Mitchell's Tennis Example");
             
               data.Columns.Add("Day", "Outlook", "Temperature", "Humidity", "Wind", "PlayTennis");
            
               // We will set Temperature and Humidity to be continuous
               data.Columns["Temperature"].DataType = typeof(double); // (degrees celsius)
               data.Columns["Humidity"].DataType    = typeof(double); // (water percentage)
             
               data.Rows.Add(   "D1",   "Sunny",      38.0,         96.0,    "Weak",     "No"  );
               data.Rows.Add(   "D2",   "Sunny",      39.0,         90.0,   "Strong",    "No"  );
               data.Rows.Add(   "D3",  "Overcast",    38.0,         75.0,    "Weak",     "Yes" );
               data.Rows.Add(   "D4",   "Rain",       25.0,         87.0,    "Weak",     "Yes" );
               data.Rows.Add(   "D5",   "Rain",       12.0,         30.0,    "Weak",     "Yes" );
               data.Rows.Add(   "D6",   "Rain",       11.0,         35.0,   "Strong",    "No"  );
               data.Rows.Add(   "D7",  "Overcast",    10.0,         40.0,   "Strong",    "Yes" );
               data.Rows.Add(   "D8",   "Sunny",      24.0,         90.0,    "Weak",     "No"  );
               data.Rows.Add(   "D9",   "Sunny",      12.0,         26.0,    "Weak",     "Yes" );
               data.Rows.Add(   "D10",  "Rain",       25.0,         30.0,    "Weak",     "Yes" );
               data.Rows.Add(   "D11",  "Sunny",      26.0,         40.0,   "Strong",    "Yes" );
               data.Rows.Add(   "D12", "Overcast",    27.0,         97.0,   "Strong",    "Yes" );
               data.Rows.Add(   "D13", "Overcast",    39.0,         41.0,    "Weak",     "Yes" );
               data.Rows.Add(   "D14",  "Rain",       23.0,         98.0,   "Strong",    "No"  );
             </code>
             <para>
               <i>Obs: The DataTable representation is not required, and instead the NaiveBayes could
               also be trained directly on double[] arrays containing the data.</i></para>
               
             <para>
               In order to estimate a discrete Naive Bayes, we will first convert this problem to a more simpler
               representation. Since some variables are categories, it does not matter if they are represented
               as strings, or numbers, since both are just symbols for the event they represent. Since numbers
               are more easily representable than text strings, we will convert the problem to use a discrete 
               alphabet through the use of a <see cref="T:Accord.Statistics.Filters.Codification">codebook</see>.</para>
             
             <para>
               A codebook effectively transforms any distinct possible value for a variable into an integer 
               symbol. For example, “Sunny” could as well be represented by the integer label 0, “Overcast” 
               by “1”, Rain by “2”, and the same goes by for the other variables. So:</para>
             
             <code>
               // Create a new codification codebook to 
               // convert strings into integer symbols
               Codification codebook = new Codification(data);
               
               // Translate our training data into integer symbols using our codebook:
               DataTable symbols = codebook.Apply(data); 
               double[][] inputs  = symbols.ToArray("Outlook", "Temperature", "Humidity", "Wind"); 
               int[]      outputs = symbols.ToIntArray("PlayTennis").GetColumn(0);
             </code>
             
             <para>
               Now that we already have our learning input/ouput pairs, we should specify our
               decision tree. We will be trying to build a tree to predict the last column, entitled
               “PlayTennis”. For this, we will be using the “Outlook”, “Temperature”, “Humidity” and
               “Wind” as predictors (variables which will we will use for our decision).
             </para>
             
             <code>
               // Gather information about decision variables
               IUnivariateDistribution[] priors =
               {
                   new GeneralDiscreteDistribution(codebook["Outlook"].Symbols),   // 3 possible values (Sunny, overcast, rain)
                   new NormalDistribution(),                                       // Continuous value (celsius)
                   new NormalDistribution(),                                       // Continuous value (percentage)
                   new GeneralDiscreteDistribution(codebook["Wind"].Symbols)       // 2 possible values (Weak, strong)
               };
               
               int inputCount = 4;       // 4 variables (Outlook, Temperature, Humidity, Wind)
               int classCount = codebook["PlayTennis"].Symbols; // 2 possible values (yes, no)
            
               // Create a new Naive Bayes classifiers for the two classes
               var target = new NaiveBayes&lt;IUnivariateDistribution&gt;(classCount, inputCount, priors);
            
               // Compute the Naive Bayes model
               target.Estimate(inputs, outputs);
             </code>
             
             <para>Now that we have created and estimated our classifier, we 
             can query the classifier for new input samples through the <see cref="M:Accord.MachineLearning.Bayes.NaiveBayes`1.Compute(System.Double[])"/> method.</para>
             </example>
             
             <seealso cref="T:Accord.MachineLearning.Bayes.NaiveBayes"/>
             
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.#ctor(System.Int32,System.Int32,`0)">
            <summary>
              Constructs a new Naïve Bayes Classifier.
            </summary>
            
            <param name="classes">The number of output classes.</param>
            <param name="inputs">The number of input variables.</param>
            <param name="prior">
              A value probability prior to be used in the estimation
              of each class-variable relationship. This value will be
              replicated for each entry in the <see cref="P:Accord.MachineLearning.Bayes.NaiveBayes`1.Distributions"/>
              property.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.#ctor(System.Int32,System.Int32,`0,System.Double[])">
            <summary>
              Constructs a new Naïve Bayes Classifier.
            </summary>
            
            <param name="classes">The number of output classes.</param>
            <param name="inputs">The number of input variables.</param>
            <param name="prior">
              A value probability prior to be used in the estimation
              of each class-variable relationship. This value will be
              replicated for each entry in the <see cref="P:Accord.MachineLearning.Bayes.NaiveBayes`1.Distributions"/>
              property.</param>
            <param name="classPriors">The prior probabilities for each output class.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.#ctor(System.Int32,System.Int32,`0[])">
            <summary>
              Constructs a new Naïve Bayes Classifier.
            </summary>
            
            <param name="classes">The number of output classes.</param>
            <param name="inputs">The number of input variables.</param>
            <param name="inputPriors">
              A value probability prior for each input variable, to be used
              in the estimation of each class-variable relationship. This value
              will be replicated for each class in the <see cref="P:Accord.MachineLearning.Bayes.NaiveBayes`1.Distributions"/>
              property.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.#ctor(System.Int32,System.Int32,`0[0:,0:])">
            <summary>
              Constructs a new Naïve Bayes Classifier.
            </summary>
            
            <param name="classes">The number of output classes.</param>
            <param name="inputs">The number of input variables.</param>
            <param name="priors">
              A value probability prior for each class and input variables, to
              be used in the estimation of each class-variable relationship.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.#ctor(System.Int32,System.Int32,`0[0:,0:],System.Double[])">
            <summary>
              Constructs a new Naïve Bayes Classifier.
            </summary>
            
            <param name="classes">The number of output classes.</param>
            <param name="inputs">The number of input variables.</param>
            <param name="priors">
              A value probability prior for each class and input variables, to
              be used in the estimation of each class-variable relationship.</param>
            <param name="classPriors">The prior probabilities for each output class.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.Estimate(System.Double[][],System.Int32[],System.Boolean,Accord.Statistics.Distributions.Fitting.IFittingOptions)">
            <summary>
              Initializes the frequency tables of a Naïve Bayes Classifier.
            </summary>
            
            <param name="inputs">The input data.</param>
            <param name="outputs">The corresponding output labels for the input data.</param>
            <param name="empirical">True to estimate class priors from the data, false otherwise.</param>
            <param name="options">The fitting options to be used in the density estimation.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.Error(System.Double[][],System.Int32[])">
            <summary>
              Computes the error when predicting the given data.
            </summary>
            
            <param name="inputs">The input values.</param>
            <param name="outputs">The output values.</param>
            
            <returns>The percentual error of the prediction.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.Compute(System.Double[])">
            <summary>
              Computes the most likely class for a given instance.
            </summary>
            
            <param name="input">The input instance.</param>
            <returns>The most likely class for the instance.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.Bayes.NaiveBayes`1.Compute(System.Double[],System.Double@,System.Double[]@)">
            <summary>
              Computes the most likely class for a given instance.
            </summary>
            
            <param name="input">The input instance.</param>
            <param name="logLikelihood">The log-likelihood for the instance.</param>
            <param name="responses">The response probabilities for each class.</param>
            <returns>The most likely class for the instance.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes`1.ClassCount">
            <summary>
              Gets the number of possible output classes.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes`1.InputCount">
            <summary>
              Gets the number of inputs in the model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes`1.Distributions">
            <summary>
              Gets the probability distributions for each class and input.
            </summary>
            
            <value>A TDistribution[,] array in with each row corresponds to a 
            class, each column corresponds to an input variable. Each element
            of this double[,] array is a a probability distribution modelling
            the occurance of the input variable in the corresponding class.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.Bayes.NaiveBayes`1.Priors">
            <summary>
              Gets the prior beliefs for each class.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.SupportVectorMachineLearningConfigurationFunction">
            <summary>
              Configuration function to configure the learning algorithms
              for each of the Kernel Support Vector Machines used in this
              Multi-class Support Vector Machine.
            </summary>
            
            <param name="inputs">The input data for the learning algorithm.</param>
            <param name="outputs">The output data for the learning algorithm.</param>
            <param name="machine">The machine for the learning algorithm.</param>
            <param name="class1">The class index corresponding to the negative values
                in the output values contained in <paramref name="outputs"/>.</param>
            <param name="class2">The class index corresponding to the positive values
                in the output values contained in <paramref name="outputs"/>.</param>
                
            <returns>
              The configured <see cref="T:Accord.MachineLearning.VectorMachines.Learning.ISupportVectorMachineLearning"/> algorithm
              to be used to train the given <see cref="T:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine"/>.
            </returns>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs">
            <summary>
              Subproblem progress event argument.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs.#ctor(System.Int32,System.Int32)">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs"/> class.
            </summary>
            
            <param name="class1">One of the classes in the subproblem.</param>
            <param name="class2">The other class in the subproblem.</param>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs.Class1">
            <summary>
              One of the classes belonging to the subproblem.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs.Class2">
            <summary>
             One of the classes belonging to the subproblem.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs.Progress">
            <summary>
              Gets the progress of the overall problem,
              ranging from zero up to <see cref="P:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs.Maximum"/>.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs.Maximum">
            <summary>
              Gets the maximum value for the current <see cref="P:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs.Progress"/>.
            </summary>
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning">
             <summary>
               One-against-one Multi-class Support Vector Machine Learning Algorithm
             </summary>
             
             <remarks>
               This class can be used to train Kernel Support Vector Machines with
               any algorithm using a one-against-one strategy. The underlying training
               algorithm can be configured by defining the Configure delegate.
             </remarks>
             
             <example>
               <code>
               // Sample data
               //   The following is simple auto association function
               //   where each input correspond to its own class. This
               //   problem should be easily solved by a Linear kernel.
            
               // Sample input data
               double[][] inputs =
               {
                   new double[] { 0 },
                   new double[] { 3 },
                   new double[] { 1 },
                   new double[] { 2 },
               };
               
               // Output for each of the inputs
               int[] outputs = { 0, 3, 1, 2 };
               
               
               // Create a new Linear kernel
               IKernel kernel = new Linear();
               
               // Create a new Multi-class Support Vector Machine with one input,
               //  using the linear kernel and for four disjoint classes.
               var machine = new MulticlassSupportVectorMachine(1, kernel, 4);
               
               // Create the Multi-class learning algorithm for the machine
               var teacher = new MulticlassSupportVectorLearning(machine, inputs, outputs);
               
               // Configure the learning algorithm to use SMO to train the
               //  underlying SVMs in each of the binary class subproblems.
               teacher.Algorithm = (svm, classInputs, classOutputs, i, j) =>
                   new SequentialMinimalOptimization(svm, classInputs, classOutputs);
               
               // Run the learning algorithm
               double error = teacher.Run();
               </code>
             </example>
             
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.#ctor(Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine,System.Double[][],System.Int32[])">
            <summary>
              Constructs a new Multi-class Support Vector Learning algorithm.
            </summary>
            
            <param name="inputs">The input learning vectors for the machine learning algorithm.</param>
            <param name="machine">The <see cref="T:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine"/> to be trained.</param>
            <param name="outputs">The output labels associated with each of the input vectors. The
            class labels should be between 0 and the <see cref="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Classes">
            number of classes in the multiclass machine</see>.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.Run">
            <summary>
              Runs the one-against-one learning algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.Run(System.Boolean)">
            <summary>
              Runs the one-against-one learning algorithm.
            </summary>
            
            <param name="computeError">
              True to compute error after the training
              process completes, false otherwise. Default is true.
            </param>
            
            <returns>
              The sum of squares error rate for
              the resulting support vector machine.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.ComputeError(System.Double[][],System.Int32[])">
            <summary>
              Compute the error ratio.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.OnSubproblemFinished(Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs)">
            <summary>
              Raises the <see cref="E:SubproblemFinished"/> event.
            </summary>
            
            <param name="args">The <see cref="T:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs"/> instance containing the event data.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.OnSubproblemStarted(Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs)">
            <summary>
              Raises the <see cref="E:SubproblemStarted"/> event.
            </summary>
            
            <param name="args">The <see cref="T:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs"/> instance containing the event data.</param>
            
        </member>
        <member name="E:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.SubproblemStarted">
            <summary>
              Occurs when the learning of a subproblem has started.
            </summary>
            
        </member>
        <member name="E:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.SubproblemFinished">
            <summary>
              Occurs when the learning of a subproblem has finished.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning.Algorithm">
            <summary>
              Gets or sets the configuration function for the learning algorithm.
            </summary>
            
            <remarks>
              The configuration function should return a properly configured ISupportVectorMachineLearning
              algorithm using the given support vector machine and the input and output data.
            </remarks>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.Learning.ID3Learning">
             <summary>
               ID3 (Iterative Dichotomiser 3) learning algorithm
               for <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionTree">Decision Trees</see>.
             </summary>
             
             <remarks>
             <para>
               References:
               <list type="bullet">
                 <item><description>
                   Quinlan, J. R 1986. Induction of Decision Trees.
                   Mach. Learn. 1, 1 (Mar. 1986), 81-106.</description></item>
                 <item><description>
                   Mitchell, T. M. Machine Learning. McGraw-Hill, 1997. pp. 55-58. </description></item>
                 <item><description><a href="http://en.wikipedia.org/wiki/ID3_algorithm">
                   Wikipedia, the free enclyclopedia. ID3 algorithm. Available on 
                   http://en.wikipedia.org/wiki/ID3_algorithm </a></description></item>
               </list>
             </para>   
             </remarks>
             
             <example>
             <para>
               In this example, we will be using the famous Play Tennis example by Tom Mitchell (1998).
               In Mitchell's example, one would like to infer if a person would play tennis or not
               based solely on four input variables. Those variables are all categorical, meaning that
               there is no order between the possible values for the variable (i.e. there is no order
               relationship between Sunny and Rain, one is not bigger nor smaller than the other, but are 
               just distinct). Moreover, the rows, or instances presented above represent days on which the
               behavior of the person has been registered and annotated, pretty much building our set of 
               observation instances for learning:</para>
             
             <code>
               DataTable data = new DataTable("Mitchell's Tennis Example");
               
               data.Columns.Add("Day", "Outlook", "Temperature", "Humidity", "Wind", "PlayTennis");
               
               data.Rows.Add(   "D1",   "Sunny",      "Hot",       "High",   "Weak",    "No"  );
               data.Rows.Add(   "D2",   "Sunny",      "Hot",       "High",  "Strong",   "No"  ); 
               data.Rows.Add(   "D3",  "Overcast",    "Hot",       "High",   "Weak",    "Yes" );
               data.Rows.Add(   "D4",   "Rain",       "Mild",      "High",   "Weak",    "Yes" ); 
               data.Rows.Add(   "D5",   "Rain",       "Cool",     "Normal",  "Weak",    "Yes" ); 
               data.Rows.Add(   "D6",   "Rain",       "Cool",     "Normal", "Strong",   "No"  ); 
               data.Rows.Add(   "D7",  "Overcast",    "Cool",     "Normal", "Strong",   "Yes" );
               data.Rows.Add(   "D8",   "Sunny",      "Mild",      "High",   "Weak",    "No"  );  
               data.Rows.Add(   "D9",   "Sunny",      "Cool",     "Normal",  "Weak",    "Yes" ); 
               data.Rows.Add(   "D10", "Rain",        "Mild",     "Normal",  "Weak",    "Yes" ); 
               data.Rows.Add(   "D11",  "Sunny",      "Mild",     "Normal", "Strong",   "Yes" );
               data.Rows.Add(   "D12", "Overcast",    "Mild",      "High",  "Strong",   "Yes" ); 
               data.Rows.Add(   "D13", "Overcast",    "Hot",      "Normal",  "Weak",    "Yes" ); 
               data.Rows.Add(   "D14",  "Rain",       "Mild",      "High",  "Strong",   "No"  );
             </code>
             
             <para>
               In order to try to learn a decision tree, we will first convert this problem to a more simpler
               representation. Since all variables are categories, it does not matter if they are represented
               as strings, or numbers, since both are just symbols for the event they represent. Since numbers
               are more easily representable than text string, we will convert the problem to use a discrete 
               alphabet through the use of a <see cref="T:Accord.Statistics.Filters.Codification">codebook</see>.</para>
             
             <para>
               A codebook effectively transforms any distinct possible value for a variable into an integer 
               symbol. For example, “Sunny” could as well be represented by the integer label 0, “Overcast” 
               by “1”, Rain by “2”, and the same goes by for the other variables. So:</para>
             
             <code>
               // Create a new codification codebook to 
               // convert strings into integer symbols
               Codification codebook = new Codification(data);
               
               // Translate our training data into integer symbols using our codebook:
               DataTable symbols = codebook.Apply(data); 
               int[][] inputs  = symbols.ToIntArray("Outlook", "Temperature", "Humidity", "Wind"); 
               int[]   outputs = symbols.ToIntArray("PlayTennis").GetColumn(0);
             </code>
             
             <para>
               Now that we already have our learning input/ouput pairs, we should specify our
               decision tree. We will be trying to build a tree to predict the last column, entitled
               “PlayTennis”. For this, we will be using the “Outlook”, “Temperature”, “Humidity” and
               “Wind” as predictors (variables which will we will use for our decision). Since those
               are categorical, we must specify, at the moment of creation of our tree, the
               characteristics of each of those variables. So:
             </para>
             
             <code>
               // Gather information about decision variables
               DecisionVariable[] attributes =
               {
                 new DecisionVariable("Outlook",     3), // 3 possible values (Sunny, overcast, rain)
                 new DecisionVariable("Temperature", 3), // 3 possible values (Hot, mild, cool)  
                 new DecisionVariable("Humidity",    2), // 2 possible values (High, normal)    
                 new DecisionVariable("Wind",        2)  // 2 possible values (Weak, strong) 
               };
               
               int classCount = 2; // 2 possible output values for playing tennis: yes or no
            
               //Create the decision tree using the attributes and classes
               DecisionTree tree = new DecisionTree(attributes, classCount); 
             </code>
             
             <para>Now we have created our decision tree. Unfortunately, it is not really very useful,
             since we haven't taught it the problem we are trying to predict. So now we must instantiate
             a learning algorithm to make it useful. For this task, in which we have only categorical 
             variables, the simplest choice is to use the ID3 algorithm by Quinlan. Let’s do it:</para>
             
             <code>
               // Create a new instance of the ID3 algorithm
               ID3Learning id3learning = new ID3Learning(tree);
            
               // Learn the training instances!
               id3learning.Run(inputs, outputs); 
             </code>
             
             <para>The tree can now be queried for new examples through its <see cref="M:Accord.MachineLearning.DecisionTrees.DecisionTree.Compute(System.Double[])"/> method.
             </para>
             </example>
             
            
             <see cref="T:Accord.MachineLearning.DecisionTrees.Learning.C45Learning"/>
             
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.Learning.ID3Learning.#ctor(Accord.MachineLearning.DecisionTrees.DecisionTree)">
            <summary>
              Creates a new ID3 learning algorithm.
            </summary>
            
            <param name="tree">The decision tree to be generated.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.Learning.ID3Learning.Run(System.Int32[][],System.Int32[])">
            <summary>
              Runs the learning algorithm, creating a decision
              tree modeling the given inputs and outputs.
            </summary>
            
            <param name="inputs">The inputs.</param>
            <param name="outputs">The corresponding outputs.</param>
            
            <returns>The error of the generated tree.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.Learning.ID3Learning.ComputeError(System.Int32[][],System.Int32[])">
            <summary>
              Computes the prediction error for the tree
              over a given set of input and outputs.
            </summary>
            
            <param name="inputs">The input points.</param>
            <param name="outputs">The corresponding output labels.</param>
            
            <returns>The percentual error of the prediction.</returns>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.DecisionNode">
            <summary>
              Decision Tree (DT) Node.
            </summary>
            
            <remarks>
              Each node of a decision tree can play two roles. When a node is not a leaf, it
              contains a <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionBranchNodeCollection"/> with a collection of child nodes. The
              branch specifies an attribute index, indicating which column from the data set
              (the attribute) should be compared against its children values. The type of the
              comparison is specified by each of the children. When a node is a leaf, it will
              contain the output value which should be decided for when the node is reached.
            </remarks>
            
            <seealso cref="T:Accord.MachineLearning.DecisionTrees.DecisionTree"/>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionNode.#ctor(Accord.MachineLearning.DecisionTrees.DecisionTree)">
            <summary>
              Creates a new decision node.
            </summary>
            
            <param name="owner">The owner tree for this node.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionNode.Compute(System.Double)">
            <summary>
              Computes whether a value satisfies
              the condition imposed by this node.
            </summary>
            
            <param name="x">The value x.</param>
            
            <returns><c>true</c> if the value satisfies this node's
            condition; otherwise, <c>false</c>.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionNode.ToString">
            <summary>
              Returns a <see cref="T:System.String"/> that represents this instance.
            </summary>
            
            <returns>
              A <see cref="T:System.String"/> that represents this instance.
            </returns>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionNode.Value">
            <summary>
              Gets or sets the value this node responds to
              whenever this node acts as a child node. This
              value is set only when the node has a parent.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionNode.Comparison">
            <summary>
              Gets or sets the type of the comparison which
              should be done against <see cref="P:Accord.MachineLearning.DecisionTrees.DecisionNode.Value"/>.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionNode.Output">
            <summary>
              If this is a leaf node, gets or sets the output
              value to be decided when this node is reached.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionNode.Branches">
            <summary>
              If this is not a leaf node, gets or sets the collection
              of child nodes for this node, together with the attribute
              determining the reasoning process for those children.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionNode.Parent">
            <summary>
              Gets or sets the parent of this node. If this is a root
              node, the parent is <c>null</c>.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionNode.Owner">
            <summary>
              Gets the <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionTree"/> containing this node.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionNode.IsRoot">
            <summary>
              Gets a value indicating whether this instance is a root node (has no parent).
            </summary>
            
            <value><c>true</c> if this instance is a root; otherwise, <c>false</c>.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionNode.IsLeaf">
            <summary>
              Gets a value indicating whether this instance is a leaf (has no children).
            </summary>
            
            <value><c>true</c> if this instance is a leaf; otherwise, <c>false</c>.</value>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.DecisionBranchNodeCollection">
            <summary>
              Collection of decision nodes. A decision branch specifies the index of
              an attribute whose current value should be compared against its children
              nodes. The type of the comparison is specified in each child node.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionBranchNodeCollection.#ctor(System.Int32,Accord.MachineLearning.DecisionTrees.DecisionNode[])">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionBranchNodeCollection"/> class.
            </summary>
            
            <param name="attributeIndex">Index of the attribute to be processed.</param>
            
            <param name="nodes">The children nodes. Each child node should be
            responsable for a possible value of a discrete attribute, or for
            a region of a continuous-valued attribute.</param>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionBranchNodeCollection.AttributeIndex">
            <summary>
              Gets or sets the index of the attribute to be
              used in this stage of the decisioning process.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.ComparisonKind">
            <summary>
              Numeric comparison category.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.ComparisonKind.None">
            <summary>
              The node does no comparison.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.ComparisonKind.Equal">
            <summary>
              The node compares for equality.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.ComparisonKind.NotEqual">
            <summary>
              The node compares for non-equality.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.ComparisonKind.GreaterThanOrEqual">
            <summary>
              The node compares for greater-than or equality.
            </summary>
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.ComparisonKind.GreaterThan">
            <summary>
              The node compares for greater-than.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.ComparisonKind.LessThan">
            <summary>
              The node compares for less-than.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.ComparisonKind.LessThanOrEqual">
            <summary>
              The node compares for less-than or equality.
            </summary>
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.SelectionStrategy">
            <summary>
              Gets the selection strategy to be used in SMO.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.VectorMachines.Learning.SelectionStrategy.Sequential">
            <summary>
              Uses the sequential selection strategy as
               suggested by Keerthi et al's algorithm 1.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.VectorMachines.Learning.SelectionStrategy.WorstPair">
            <summary>
              Always select the worst violation pair
              to be optimized first, as suggested in
              Keerthy et al's algorithm 2.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization">
            <summary>
              Sequential Minimal Optimization (SMO) Algorithm
            </summary>
            
            <remarks>
            <para>
              The SMO algorithm is an algorithm for solving large quadratic programming (QP)
              optimization problems, widely used for the training of support vector machines.
              First developed by John C. Platt in 1998, SMO breaks up large QP problems into
              a series of smallest possible QP problems, which are then solved analytically.</para>
            <para>
              This class follows the original algorithm by Platt with additional modifications
              by Keerthi et al.</para>
             
            <para>
              References:
              <list type="bullet">
                <item><description>
                  <a href="http://en.wikipedia.org/wiki/Sequential_Minimal_Optimization">
                  Wikipedia, The Free Encyclopedia. Sequential Minimal Optimization. Available on:
                  http://en.wikipedia.org/wiki/Sequential_Minimal_Optimization </a></description></item>
                <item><description>
                  <a href="http://research.microsoft.com/en-us/um/people/jplatt/smoTR.pdf">
                  John C. Platt, Sequential Minimal Optimization: A Fast Algorithm for Training Support
                  Vector Machines. 1998. Available on: http://research.microsoft.com/en-us/um/people/jplatt/smoTR.pdf </a></description></item>
                <item><description>
                  <a href="http://www.cs.iastate.edu/~honavar/keerthi-svm.pdf">
                  S. S. Keerthi et al. Improvements to Platt's SMO Algorithm for SVM Classifier Design.
                  Technical Report CD-99-14. Available on: http://www.cs.iastate.edu/~honavar/keerthi-svm.pdf </a></description></item>
                <item><description>
                  <a href="http://www.idiom.com/~zilla/Work/Notes/svmtutorial.pdf">
                  J. P. Lewis. A Short SVM (Support Vector Machine) Tutorial. Available on:
                  http://www.idiom.com/~zilla/Work/Notes/svmtutorial.pdf </a></description></item>
                </list></para>  
            </remarks>
            
            <example>
              <code>
              // Example XOR problem
              double[][] inputs =
              {
                  new double[] { 0, 0 }, // 0 xor 0: 1 (label +1)
                  new double[] { 0, 1 }, // 0 xor 1: 0 (label -1)
                  new double[] { 1, 0 }, // 1 xor 0: 0 (label -1)
                  new double[] { 1, 1 }  // 1 xor 1: 1 (label +1)
              };
               
              // Dichotomy SVM outputs should be given as [-1;+1]
              int[] labels =
              {
                     1, -1, -1, 1
              };
             
              // Create a Kernel Support Vector Machine for the given inputs
              KernelSupportVectorMachine machine = new KernelSupportVectorMachine(new Gaussian(0.1), inputs[0].Length);
            
              // Instantiate a new learning algorithm for SVMs
              SequentialMinimalOptimization smo = new SequentialMinimalOptimization(machine, inputs, labels);
            
              // Set up the learning algorithm
              smo.Complexity = 1.0;
            
              // Run the learning algorithm
              double error = smo.Run();
              
              // Compute the decision output for one of the input vectors
              int decision = System.Math.Sign(svm.Compute(inputs[0]));
             </code>
            </example>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.#ctor(Accord.MachineLearning.VectorMachines.SupportVectorMachine,System.Double[][],System.Int32[])">
            <summary>
              Initializes a new instance of a Sequential Minimal Optimization (SMO) algorithm.
            </summary>
            
            <param name="machine">A Support Vector Machine.</param>
            <param name="inputs">The input data points as row vectors.</param>
            <param name="outputs">The outout label for each input point. Values must be either -1 or +1.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Run(System.Boolean)">
            <summary>
              Runs the SMO algorithm.
            </summary>
            
            <param name="computeError">
              True to compute error after the training
              process completes, false otherwise. Default is true.
            </param>
            
            <returns>
              The misclassification error rate of
              the resulting support vector machine.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Run">
            <summary>
              Runs the SMO algorithm.
            </summary>
            
            <returns>
              The misclassification error rate of
              the resulting support vector machine.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.ComputeError(System.Double[][],System.Int32[])">
            <summary>
              Computes the error rate for a given set of input and outputs.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.examineExample(System.Int32)">
            <summary>
             Chooses which multipliers to optimize using heuristics.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.takeStep(System.Int32,System.Int32)">
            <summary>
              Analytically solves the optimization problem for two Lagrange multipliers.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.compute(System.Double[])">
            <summary>
              Computes the SVM output for a given point.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.computeNoBias(System.Int32)">
            <summary>
              Computes the SVM output for a given point.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.EstimateComplexity(Accord.Statistics.Kernels.IKernel,System.Double[][])">
            <summary>
              Estimates the <see cref="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Complexity">complexity parameter C</see>
              for a given kernel and a given data set.
            </summary>
            
            <param name="kernel">The kernel function.</param>
            <param name="inputs">The input samples.</param>
            
            <returns>A suitable value for C.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Complexity">
            <summary>
              Complexity (cost) parameter C. Increasing the value of C forces the creation
              of a more accurate model that may not generalize well. Default value is the
              number of examples divided by the trace of the kernel matrix.
            </summary>
            
            <remarks>
              The cost parameter C controls the trade off between allowing training
              errors and forcing rigid margins. It creates a soft margin that permits
              some misclassifications. Increasing the value of C increases the cost of
              misclassifying points and forces the creation of a more accurate model
              that may not generalize well.
            </remarks>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.UseComplexityHeuristic">
            <summary>
              Gets or sets a value indicating whether the Complexity parameter C
              should be computed automatically by employing an heuristic rule.
            </summary>
            <value>
            	<c>true</c> if complexity should be computed automatically; otherwise, <c>false</c>.
            </value>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Epsilon">
            <summary>
              Epsilon for round-off errors. Default value is 1e-12.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Tolerance">
            <summary>
              Convergence tolerance. Default value is 1e-2.
            </summary>
            
            <remarks>
              The criterion for completing the model training process. The default is 0.01.
            </remarks>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Strategy">
            <summary>
              Gets or sets the <see cref="T:Accord.MachineLearning.VectorMachines.Learning.SelectionStrategy">pair selection 
              streategy</see> to be used during optimization.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.CacheSize">
            <summary>
              Gets or sets the cache size to partially
              stored the kernel matrix. Default is the
              same number of input vectors.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Lagrange">
            <summary>
              Gets the value for the Lagrange multipliers
              (alpha) for every observation vector.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.Compact">
            <summary>
              Gets or sets whether to produce compact models. Compact
              formulation is currently limited to linear models.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.ActiveExamples">
            <summary>
              Gets the indices of the active examples (examples which have
              the corresponding Lagrange multiplier different than zero).
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.NonBoundExamples">
            <summary>
              Gets the indices of the non-bounded examples (examples which
              have the corresponding Lagrange multipliers between 0 and C).
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.SequentialMinimalOptimization.BoundedExamples">
            <summary>
              Gets the indices of the examples at the boundary (examples
              which have the corresponding Lagrange multipliers equal to C).
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KMeans">
             <summary>
               K-Means algorithm.
             </summary>
             
             <remarks>
             <para>
               In statistics and machine learning, k-means clustering is a method
               of cluster analysis which aims to partition n observations into k 
               clusters in which each observation belongs to the cluster with the
               nearest mean.</para>
             <para>
               It is similar to the expectation-maximization algorithm for mixtures
               of Gaussians in that they both attempt to find the centers of natural
               clusters in the data as well as in the iterative refinement approach
               employed by both algorithms.</para> 
             
             <para>
               The algorithm is composed of the following steps:
               <list type="number">
                 <item><description>
                     Place K points into the space represented by the objects that are
                     being clustered. These points represent initial group centroids.
                 </description></item>
                 <item><description>
                     Assign each object to the group that has the closest centroid.
                 </description></item>
                 <item><description>
                     When all objects have been assigned, recalculate the positions
                     of the K centroids.
                 </description></item>
                 <item><description>
                     Repeat Steps 2 and 3 until the centroids no longer move. This
                     produces a separation of the objects into groups from which the
                     metric to be minimized can be calculated.
                 </description></item>
               </list></para>
             
             <para>
               This particular implementation uses the squared euclidean distance
               as a similarity measure in order to form clusters. </para>
               
             <para>
               References:
               <list type="bullet">
                 <item><description>
                   Wikipedia, The Free Encyclopedia. K-means clustering. Available on:
                   http://en.wikipedia.org/wiki/K-means_clustering </description></item>
                 <item><description>
                   Matteo Matteucci. A Tutorial on Clustering Algorithms. Available on:
                   http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/kmeans.html </description></item>
               </list></para>
             </remarks>
             <example>
               How to perform clustering with K-Means.
               <code>
               // Declare some observations
               double[][] observations = 
               {
                   new double[] { -5, -2, -1 },
                   new double[] { -5, -5, -6 },
                   new double[] {  2,  1,  1 },
                   new double[] {  1,  1,  2 },
                   new double[] {  1,  2,  2 },
                   new double[] {  3,  1,  2 },
                   new double[] { 11,  5,  4 },
                   new double[] { 15,  5,  6 },
                   new double[] { 10,  5,  6 },
               };
              
               // Create a new K-Means algorithm with 3 clusters 
               KMeans kmeans = new KMeans(3);
              
               // Compute the algorithm, retrieving an integer array
               //  containing the labels for each of the observations
               int[] labels = kmeans.Compute(observations);
              
               // As result, the first two observations should belong to the
               // same cluster (thus having the same label). The same should
               // happen to the next four observations and to the last three.
               </code>
             </example>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.#ctor(System.Int32)">
            <summary>
              Initializes a new instance of K-Means algorithm
            </summary>
            
            <param name="k">The number of clusters to divide input data.</param>    
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.#ctor(System.Int32,System.Func{System.Double[],System.Double[],System.Double})">
            <summary>
              Initializes a new instance of KMeans algorithm
            </summary>
            
            <param name="k">The number of clusters to divide input data.</param>       
            <param name="distance">The distance function to use. Default is to
            use the <see cref="M:Accord.Math.Distance.SquareEuclidean(System.Double[],System.Double[])"/> distance.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Randomize(System.Double[][],System.Boolean)">
            <summary>
              Randomizes the clusters inside a dataset.
            </summary>
            
            <param name="data">The data to randomize the algorithm.</param>
            <param name="useSeeding">True to use the k-means++ seeding algorithm. False otherwise.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Compute(System.Double[][])">
            <summary>
              Divides the input data into K clusters. 
            </summary>     
            
            <param name="data">The data where to compute the algorithm.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Compute(System.Double[][],System.Double@)">
            <summary>
              Divides the input data into K clusters. 
            </summary>    
            
            <param name="data">The data where to compute the algorithm.</param>
            <param name="error">
              The average square distance from the
              data points to the clusters' centroids.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Compute(System.Double[][],System.Double,System.Boolean)">
            <summary>
              Divides the input data into K clusters. 
            </summary>     
            
            <param name="data">The data where to compute the algorithm.</param>
            <param name="threshold">The relative convergence threshold
              for the algorithm. Default is 1e-5.</param>
            <param name="computeInformation">Pass <c>true</c> to compute additional information
              when the algorithm finishes, such as cluster variances and proportions; false
              otherwise. Default is true.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Compute(System.Double[][],System.Double,System.Double@)">
            <summary>
              Divides the input data into K clusters. 
            </summary>  
            
            <param name="data">The data where to compute the algorithm.</param>
            <param name="threshold">The relative convergence threshold
            for the algorithm. Default is 1e-5.</param>
            
            <param name="error">
              The average square distance from the
              data points to the clusters' centroids.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Nearest(System.Double[])">
            <summary>
              Returns the closest cluster to an input vector.
            </summary>
            
            <param name="point">The input vector.</param>
            <returns>
              The index of the nearest cluster
              to the given data point. </returns>
              
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Nearest(System.Double[][])">
            <summary>
              Returns the closest clusters to an input vector array.
            </summary>
            
            <param name="points">The input vector array.</param>
            
            <returns>
              An array containing the index of the nearest cluster
              to the corresponding point in the input array.</returns>
              
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Error(System.Double[][])">
            <summary>
              Calculates the average square distance from the data points
              to the clusters' centroids.
            </summary>
            
            <remarks>
              The average distance from centroids can be used as a measure
              of the "goodness" of the clusterization. The more the data
              are aggregated around the centroids, the less the average
              distance.
            </remarks>
            
            <returns>
              The average square distance from the data points to the
              clusters' centroids.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.Error(System.Double[][],System.Int32[])">
            <summary>
              Calculates the average square distance from the data points
              to the clusters' centroids.
            </summary>
            
            <remarks>
              The average distance from centroids can be used as a measure
              of the "goodness" of the clusterization. The more the data
              are aggregated around the centroids, the less the average
              distance.
            </remarks>
            
            <returns>
              The average square distance from the data points to the
              clusters' centroids.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.KMeans.converged(System.Double[][],System.Double[][],System.Double)">
            <summary>
              Determines if the algorithm has converged by comparing the
              centroids between two consecutive iterations.
            </summary>
            
            <param name="centroids">The previous centroids.</param>
            <param name="newCentroids">The new centroids.</param>
            <param name="threshold">A convergence threshold.</param>
            
            <returns>Returns <see langword="true"/> if all centroids had a percentage change
               less than <see param="threshold"/>. Returns <see langword="false"/> otherwise.</returns>
               
        </member>
        <member name="P:Accord.MachineLearning.KMeans.Clusters">
            <summary>
              Gets the clusters found by K-means.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeans.K">
            <summary>
              Gets the number of clusters.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeans.Dimension">
            <summary>
              Gets the dimensionality of the data space.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeans.Distance">
            <summary>
              Gets or sets the distance function used
              as a distance metric between data points.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KMeansCluster">
            <summary>
              K-means' Cluster
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeansCluster.Index">
            <summary>
              Gets the label for this cluster.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeansCluster.Mean">
            <summary>
              Gets the cluster's centroid.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeansCluster.Covariance">
            <summary>
              Gets the cluster's variance-covariance matrix.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeansCluster.Proportion">
            <summary>
              Gets the proportion of samples in the cluster.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KMeansClusterCollection">
            <summary>
              K-means Cluster Collection.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeansClusterCollection.Covariances">
            <summary>
              Gets the clusters' variance-covariance matrices.
            </summary>
            
            <value>The clusters' variance-covariance matrices.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeansClusterCollection.Centroids">
            <summary>
              Gets or sets the clusters' centroids.
            </summary>
            
            <value>The clusters' centroids.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.KMeansClusterCollection.Proportions">
            <summary>
              Gets the proportion of samples in each cluster.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.DecisionTree">
             <summary>
               Decision tree.
             </summary>
             
             <remarks>
             <para>
               Represent a decision tree which can be compiled to
               code at run-time. For sample usage and example of
               learning, please see the <see cref="T:Accord.MachineLearning.DecisionTrees.Learning.ID3Learning">
               ID3 learning algorithm for decision trees</see>.</para>
             </remarks>
            
             <seealso cref="T:Accord.MachineLearning.DecisionTrees.Learning.ID3Learning"/>
             <seealso cref="T:Accord.MachineLearning.DecisionTrees.Learning.C45Learning"/>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionTree.#ctor(Accord.MachineLearning.DecisionTrees.DecisionVariable[],System.Int32)">
            <summary>
              Creates a new <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionTree"/> to process
              the given <paramref name="attributes"/> and the given
              number of possible <paramref name="outputClasses"/>.
            </summary>
            
            <param name="attributes">An array specifying the attributes to be processed by this tree.</param>
            <param name="outputClasses">The number of possible output classes for the given atributes.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionTree.Compute(System.Double[])">
            <summary>
              Computes the decision for a given input.
            </summary>
            
            <param name="input">The input data.</param>
            
            <returns>A predicted class for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionTree.ToExpression">
            <summary>
              Creates an <see cref="T:System.Linq.Expressions.Expression">Expression Tree</see> representation
              of this decision tree, which can in turn be compiled into code.
            </summary>
            
            <returns>A tree in the form of an expression tree.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionTree.Root">
            <summary>
              Gets or sets the root node for this tree.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionTree.Attributes">
            <summary>
              Gets the collection of attributes processed by this tree.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionTree.OutputClasses">
            <summary>
              Gets the number of distinct output
              classes classified by this tree.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionTree.InputCount">
            <summary>
              Gets the number of input attributes
              expected by this tree.
            </summary>
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.DecisionAttributeKind">
            <summary>
              Attribute category.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.DecisionAttributeKind.Discrete">
            <summary>
              Attribute is discrete-valued.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.DecisionTrees.DecisionAttributeKind.Continuous">
            <summary>
              Attribute is continuous-valued.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.DecisionVariable">
            <summary>
              Decision attribute.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionVariable.#ctor(System.String,Accord.MachineLearning.DecisionTrees.DecisionAttributeKind,AForge.DoubleRange)">
            <summary>
              Creates a new <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionVariable"/>.
            </summary>
            
            <param name="name">The name of the attribute.</param>
            <param name="nature">The attribute's nature (i.e. real-valued or discrete-valued).</param>
            <param name="range">The range of valid values for this attribute. Default is [0;1].</param>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionVariable.#ctor(System.String,Accord.MachineLearning.DecisionTrees.DecisionAttributeKind)">
            <summary>
              Creates a new <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionVariable"/>.
            </summary>
            
            <param name="name">The name of the attribute.</param>
            <param name="nature">The attribute's nature (i.e. real-valued or discrete-valued).</param>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionVariable.#ctor(System.String,AForge.IntRange)">
            <summary>
              Creates a new <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionVariable"/>.
            </summary>
            
            <param name="name">The name of the attribute.</param>
            <param name="range">The range of valid values for this attribute.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionVariable.#ctor(System.String,System.Int32)">
            <summary>
              Creates a new discrete-valued <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionVariable"/>.
            </summary>
            
            <param name="name">The name of the attribute.</param>
            <param name="symbols">The number of possible values for this attribute.</param>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionVariable.Name">
            <summary>
              Gets the name of the attribute.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionVariable.Nature">
            <summary>
              Gets the nature of the attribute (i.e. real-valued or discrete-valued).
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.DecisionTrees.DecisionVariable.Range">
            <summary>
              Gets the valid range of the attribute.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.DecisionAttributeCollection">
            <summary>
              Collection of decision attributes.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.DecisionAttributeCollection.#ctor(System.Collections.Generic.IList{Accord.MachineLearning.DecisionTrees.DecisionVariable})">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionAttributeCollection"/> class.
            </summary>
            
            <param name="list">The list to initialize the collection.</param>
            
        </member>
        <member name="T:Accord.MachineLearning.CrossValidationValues`1">
            <summary>
              Information class to store the training and validation errors of a model. 
            </summary>
            
            <typeparam name="TModel">The type of the model.</typeparam>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationValues`1.#ctor(`0,System.Double,System.Double)">
            <summary>
              Creates a new CrossvalidationInfo class.
            </summary>
            
            <param name="model">The fitted model.</param>
            <param name="trainingValue">The training value for the model.</param>
            <param name="validationValue">The validation value for the model.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.CrossValidationValues`1.#ctor(`0,System.Double,System.Double,System.Double,System.Double)">
            <summary>
              Creates a new CrossvalidationInfo class.
            </summary>
            
            <param name="model">The fitted model.</param>
            <param name="trainingValue">The training value for the model.</param>
            <param name="validationValue">The validation value for the model.</param>
            <param name="trainingVariance">The variance of the training values.</param>
            <param name="validationVariance">The variance of the validation values.</param>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationValues`1.Model">
            <summary>
              Gets the model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationValues`1.ValidationValue">
            <summary>
              Gets the validation value for the model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationValues`1.ValidationVariance">
            <summary>
              Gets the variance of the validation 
              value for the model, if available.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationValues`1.TrainingValue">
            <summary>
              Gets the training value for the model.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationValues`1.TrainingVariance">
            <summary>
              Gets the variance of the training 
              value for the model, if available.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.CrossValidationValues`1.Tag">
            <summary>
              Gets or sets a tag for user-defined information.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.DecisionTrees.Learning.C45Learning">
             <summary>
               C4.5 Learning algorithm for <see cref="T:Accord.MachineLearning.DecisionTrees.DecisionTree">Decision Trees</see>.
             </summary>
             
             <remarks>
             <para>
               References:
               <list type="bullet">
                 <item><description>
                   Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan
                   Kaufmann Publishers, 1993.</description></item>
                 <item><description>
                   Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan
                   Kaufmann Publishers, 1993.</description></item>
                 <item><description>
                   Quinlan, J. R. Improved use of continuous attributes in c4.5. Journal
                   of Artificial Intelligence Research, 4:77-90, 1996.</description></item>
                 <item><description>
                   Mitchell, T. M. Machine Learning. McGraw-Hill, 1997. pp. 55-58. </description></item>
                 <item><description><a href="http://en.wikipedia.org/wiki/ID3_algorithm">
                   Wikipedia, the free enclyclopedia. ID3 algorithm. Available on 
                   http://en.wikipedia.org/wiki/ID3_algorithm </a></description></item>
               </list>
             </para>   
             </remarks>
            
             <see cref="T:Accord.MachineLearning.DecisionTrees.Learning.ID3Learning"/>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.Learning.C45Learning.#ctor(Accord.MachineLearning.DecisionTrees.DecisionTree)">
            <summary>
              Creates a new C4.5 learning algorithm.
            </summary>
            
            <param name="tree">The decision tree to be generated.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.Learning.C45Learning.Run(System.Double[][],System.Int32[])">
            <summary>
              Runs the learning algorithm, creating a decision
              tree modeling the given inputs and outputs.
            </summary>
            
            <param name="inputs">The inputs.</param>
            <param name="outputs">The corresponding outputs.</param>
            
            <returns>The error of the generated tree.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.DecisionTrees.Learning.C45Learning.ComputeError(System.Double[][],System.Int32[])">
            <summary>
              Computes the prediction error for the tree
              over a given set of input and outputs.
            </summary>
            
            <param name="inputs">The input points.</param>
            <param name="outputs">The corresponding output labels.</param>
            
            <returns>The percentual error of the prediction.</returns>
            
        </member>
        <member name="T:Accord.MachineLearning.GridSearchParameter">
            <summary>
              Contains the name and value of a parameter that should be used during fitting.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchParameter.#ctor(System.String,System.Double)">
            <summary>
              Constructs a new parameter.
            </summary>
            
            <param name="name">The name for the parameter.</param>
            <param name="value">The value for the parameter.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchParameter.Equals(System.Object)">
            <summary>
              Determines whether the specified object is equal
              to the current GridSearchParameter object.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchParameter.GetHashCode">
            <summary>
              Returns the hash code for this GridSearchParameter
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchParameter.op_Equality(Accord.MachineLearning.GridSearchParameter,Accord.MachineLearning.GridSearchParameter)">
            <summary>
              Compares two GridSearchParameters for equality.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchParameter.op_Inequality(Accord.MachineLearning.GridSearchParameter,Accord.MachineLearning.GridSearchParameter)">
            <summary>
              Compares two GridSearchParameters for inequality.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GridSearchParameter.Name">
            <summary>
              Gets the name of the parameter
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GridSearchParameter.Value">
            <summary>
              Gets the value of the parameter.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.GridSearchParameterCollection">
            <summary>
              GridsearchParameter collection.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchParameterCollection.#ctor(Accord.MachineLearning.GridSearchParameter[])">
            <summary>
              Constructs a new collection of GridsearchParameter objects.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchParameterCollection.#ctor(System.Collections.Generic.IEnumerable{Accord.MachineLearning.GridSearchParameter})">
            <summary>
              Constructs a new collection of GridsearchParameter objects.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearchParameterCollection.GetKeyForItem(Accord.MachineLearning.GridSearchParameter)">
            <summary>
              Returns the identifying value for an item on this collection.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning">
             <summary>
               One-against-all Multi-label Support Vector Machine Learning Algorithm
             </summary>
             
             <remarks>
               This class can be used to train Kernel Support Vector Machines with
               any algorithm using a one-against-all strategy. The underlying training
               algorithm can be configured by defining the Configure delegate.
             </remarks>
             
             <example>
               <code>
               // Sample data
               //   The following is simple auto association function
               //   where each input correspond to its own class. This
               //   problem should be easily solved by a Linear kernel.
            
               // Sample input data
               double[][] inputs =
               {
                   new double[] { 0 },
                   new double[] { 3 },
                   new double[] { 1 },
                   new double[] { 2 },
               };
               
               // Outputs for each of the inputs
               int[][] outputs =
               {
                   new[] { 0, 1, 0 }
                   new[] { 0, ,  1 }
                   new[] { 1, 1, 0 }
               }
               
               
               // Create a new Linear kernel
               IKernel kernel = new Linear();
               
               // Create a new Multi-class Support Vector Machine with one input,
               //  using the linear kernel and for four disjoint classes.
               var machine = new MultilabelSupportVectorMachine(1, kernel, 4);
               
               // Create the Multi-label learning algorithm for the machine
               var teacher = new MultilabelSupportVectorLearning(machine, inputs, outputs);
               
               // Configure the learning algorithm to use SMO to train the
               //  underlying SVMs in each of the binary class subproblems.
               teacher.Algorithm = (svm, classInputs, classOutputs, i, j) =>
                   new SequentialMinimalOptimization(svm, classInputs, classOutputs);
               
               // Run the learning algorithm
               double error = teacher.Run();
               </code>
             </example>
             
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.#ctor(Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine,System.Double[][],System.Int32[][])">
            <summary>
              Constructs a new Multi-label Support Vector Learning algorithm.
            </summary>
            
            <param name="inputs">The input learning vectors for the machine learning algorithm.</param>
            <param name="machine">The <see cref="T:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine"/> to be trained.</param>
            <param name="outputs">The output labels associated with each of the input vectors. The
            class labels should be between 0 and the <see cref="P:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Classes">
            number of classes in the multiclass machine</see>. In a multi-label SVM, multiple classes
            can be associated with a single input vector.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.Run">
            <summary>
              Runs the one-against-one learning algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.Run(System.Boolean)">
            <summary>
              Runs the one-against-one learning algorithm.
            </summary>
            
            <param name="computeError">
              True to compute error after the training
              process completes, false otherwise. Default is true.
            </param>
            
            <returns>
              The sum of squares error rate for
              the resulting support vector machine.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.ComputeError(System.Double[][],System.Int32[][])">
            <summary>
              Compute the error ratio.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.OnSubproblemFinished(Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs)">
            <summary>
              Raises the <see cref="E:SubproblemFinished"/> event.
            </summary>
            
            <param name="args">The <see cref="T:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs"/> instance containing the event data.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.OnSubproblemStarted(Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs)">
            <summary>
              Raises the <see cref="E:SubproblemStarted"/> event.
            </summary>
            
            <param name="args">The <see cref="T:Accord.MachineLearning.VectorMachines.Learning.SubproblemEventArgs"/> instance containing the event data.</param>
            
        </member>
        <member name="E:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.SubproblemStarted">
            <summary>
              Occurs when the learning of a subproblem has started.
            </summary>
            
        </member>
        <member name="E:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.SubproblemFinished">
            <summary>
              Occurs when the learning of a subproblem has finished.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning.Algorithm">
            <summary>
              Gets or sets the configuration function for the learning algorithm.
            </summary>
            
            <remarks>
              The configuration function should return a properly configured ISupportVectorMachineLearning
              algorithm using the given support vector machine and the input and output data.
            </remarks>
            
        </member>
        <member name="T:Accord.MachineLearning.GaussianMixtureModel">
            <summary>
              Gaussian Mixture Model Clustering.
            </summary>
            
            <remarks>
              Gaussian Mixture Models are one of most widely used model-based 
              clustering methods. This specialized class provides a wrap-up
              around the
              <see cref="T:Accord.Statistics.Distributions.Multivariate.MultivariateMixture`1">
              Mixture&lt;NormalDistribution&gt;</see> distribution and provides
              mixture initialization using the K-Means clustering algorithm.
            </remarks>
            
            <example>
              <code>
              // Create a new Gaussian Mixture Model with 2 components
              GaussianMixtureModel gmm = new GaussianMixtureModel(2);
              
              // Compute the model (estimate)
              gmm.Compute(samples, 0.0001);
              
              // Classify a single sample
              int c = gmm.Classify(sample);
              </code>
            </example>
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.GetMixtureDistribution">
            <summary>
              Gets a mixture distribution modeled
              by this Gaussian Mixture Model.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.#ctor(System.Int32)">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.MachineLearning.GaussianMixtureModel"/> class.
            </summary>
            
            <param name="components">
              The number of clusters in the clusterization problem. This will be
              used to set the number of components in the mixture model.</param>
              
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.#ctor(Accord.MachineLearning.KMeans)">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.MachineLearning.GaussianMixtureModel"/> class.
            </summary>
            
            <param name="kmeans">
              The initial solution as a K-Means clustering.</param>
              
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Initialize(System.Double[][],System.Double)">
            <summary>
              Initializes the model with initial values obtained 
              throught a run of the K-Means clustering algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Initialize(Accord.MachineLearning.KMeans)">
            <summary>
              Initializes the model with initial values obtained 
              throught a run of the K-Means clustering algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Initialize(Accord.Statistics.Distributions.Multivariate.MultivariateNormalDistribution[])">
            <summary>
              Initializes the model with initial values.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Compute(System.Double[][])">
            <summary>
              Divides the input data into K clusters modeling each
              cluster as a multivariate Gaussian distribution. 
            </summary>     
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Compute(System.Double[][],System.Double)">
            <summary>
              Divides the input data into K clusters modeling each
              cluster as a multivariate Gaussian distribution. 
            </summary>     
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Compute(System.Double[][],System.Double,System.Double)">
            <summary>
              Divides the input data into K clusters modeling each
              cluster as a multivariate Gaussian distribution. 
            </summary>     
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Compute(System.Double[][],Accord.MachineLearning.GaussianMixtureModelOptions)">
            <summary>
              Divides the input data into K clusters modeling each
              cluster as a multivariate Gaussian distribution. 
            </summary>     
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Classify(System.Double[])">
            <summary>
              Returns the most likely clusters of an observation.
            </summary>
            
            <param name="observation">An input observation.</param>
            
            <returns>
              The index of the most likely cluster
              of the given observation. </returns>
              
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Classify(System.Double[],System.Double[]@)">
            <summary>
              Returns the most likely clusters of an observation.
            </summary>
            
            <param name="observation">An input observation.</param>
            <param name="responses">The likelihood responses for each cluster.</param>
            
            <returns>
              The index of the most likely cluster
              of the given observation. </returns>
              
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModel.Classify(System.Double[][])">
            <summary>
              Returns the most likely clusters for an array of observations.
            </summary>
            
            <param name="observations">An set of observations.</param>
            
            <returns>
              An array containing the index of the most likely cluster
              for each of the given observations. </returns>
              
        </member>
        <member name="P:Accord.MachineLearning.GaussianMixtureModel.Gaussians">
            <summary>
              Gets the Gaussian components of the mixture model.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.GaussianMixtureModelOptions">
            <summary>
              Options for Gaussian Mixture Model fitting.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianMixtureModelOptions.#ctor">
            <summary>
            Initializes a new instance of the <see cref="T:Accord.MachineLearning.GaussianMixtureModelOptions"/> class.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GaussianMixtureModelOptions.Threshold">
            <summary>
              Gets or sets the convergence criterion for the
              Expectation-Maximization algorithm. Default is 1e-3.
            </summary>
            
            <value>The convergence threshold.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.GaussianMixtureModelOptions.NormalOptions">
            <summary>
              Gets or sets the fitting options for the component
              Gaussian distributions of the mixture model.
            </summary>
            
            <value>The fitting options for inner Gaussian distributions.</value>
            
        </member>
        <member name="T:Accord.MachineLearning.GaussianCluster">
            <summary>
              Gaussian Mixture Model Cluster
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianCluster.Probability(System.Double[])">
            <summary>
              Gets the probability density function of the
              underlying Gaussian probability distribution 
              evaluated in point <c>x</c>.
            </summary>
            
            <param name="x">An observation.</param>
            
            <returns>
              The probability of <c>x</c> occurring
              in the weighted Gaussian distribution.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.GaussianCluster.GetDistribution">
            <summary>
              Gets the normal distribution associated with this cluster.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GaussianCluster.Index">
            <summary>
              Gets the label for this cluster.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GaussianCluster.Mean">
            <summary>
              Gets the cluster's mean.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GaussianCluster.Covariance">
            <summary>
              Gets the cluster's variance-covariance matrix.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GaussianCluster.Proportion">
            <summary>
              Gets the mixture coefficient for the cluster distribution.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.GaussianClusterCollection">
            <summary>
              Gaussian Mixture Model Cluster Collection.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.GridSearchFittingFunction`1">
            <summary>
              Delegate for Grid search fitting function.
            </summary>
            
            <typeparam name="TModel">The type of the model to fit.</typeparam>
            
            <param name="parameters">The collection of parameters to be used in the fitting process.</param>
            <param name="error">The error (or any other performance measure) returned by the model.</param>
            <returns>The model fitted to the data using the given parameters.</returns>
            
        </member>
        <member name="T:Accord.MachineLearning.GridSearch`1">
             <summary>
               Grid Search for automatic parameter tuning.
             </summary>
             <remarks>
               Grid Search tries to find the best combination of parameters across
               a range of possible values that produces the best fit model. If there
               are two parameters, each with 10 possible values, Grid Search will try
               an exhaustive evaluation of the model using every combination of points,
               resulting in 100 model fits.
             </remarks>
             
             <typeparam name="TModel">The type of the model to be tuned.</typeparam>
             
             <example>
               How to fit a Kernel Support Vector Machine using Grid Search.
               <code>
               // Example binary data
               double[][] inputs =
               {
                   new double[] { -1, -1 },
                   new double[] { -1,  1 },
                   new double[] {  1, -1 },
                   new double[] {  1,  1 }
               };
            
               int[] xor = // xor labels
               {
                   -1, 1, 1, -1
               };
            
               // Declare the parameters and ranges to be searched
               GridSearchRange[] ranges = 
               {
                   new GridSearchRange("complexity", new double[] { 0.00000001, 5.20, 0.30, 0.50 } ),
                   new GridSearchRange("degree",     new double[] { 1, 10, 2, 3, 4, 5 } ),
                   new GridSearchRange("constant",   new double[] { 0, 1, 2 } )
               };
            
            
               // Instantiate a new Grid Search algorithm for Kernel Support Vector Machines
               var gridsearch = new GridSearch&lt;KernelSupportVectorMachine>(ranges);
            
               // Set the fitting function for the algorithm
               gridsearch.Fitting = delegate(GridSearchParameterCollection parameters, out double error)
               {
                   // The parameters to be tried will be passed as a function parameter.
                   int degree = (int)parameters["degree"].Value;
                   double constant = parameters["constant"].Value;
                   double complexity = parameters["complexity"].Value;
            
                   // Use the parameters to build the SVM model
                   Polynomial kernel = new Polynomial(degree, constant);
                   KernelSupportVectorMachine ksvm = new KernelSupportVectorMachine(kernel, 2);
            
                   // Create a new learning algorithm for SVMs
                   SequentialMinimalOptimization smo = new SequentialMinimalOptimization(ksvm, inputs, xor);
                   smo.Complexity = complexity;
            
                   // Measure the model performance to return as an out parameter
                   error = smo.Run();
            
                   return ksvm; // Return the current model
               };
            
            
               // Declare some out variables to pass to the grid search algorithm
               GridSearchParameterCollection bestParameters; double minError;
            
               // Compute the grid search to find the best Support Vector Machine
               KernelSupportVectorMachine bestModel = gridsearch.Compute(out bestParameters, out minError);
               </code>
             </example>
             
        </member>
        <member name="M:Accord.MachineLearning.GridSearch`1.#ctor(Accord.MachineLearning.GridSearchRange[])">
            <summary>
              Constructs a new Grid search algorithm.
            </summary>
            
            <param name="parameterRanges">The range of parameters to search.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.GridSearch`1.Compute(Accord.MachineLearning.GridSearchParameterCollection@,System.Double@)">
            <summary>
              Searches for the best combination of parameters that results in the most accurate model.
            </summary>
            
            <param name="bestParameters">The best combination of parameters found by the grid search.</param>
            <param name="error">The minimum error of the best model found by the grid search.</param>
            <returns>The best model found during the grid search.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.GridSearch`1.Fitting">
            <summary>
              A function that fits a model using the given parameters.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.GridSearch`1.ParameterRanges">
            <summary>
              The range of parameters to consider during search.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KModes`1">
            <summary>
              K-Modes algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.#ctor(System.Int32,System.Func{`0[],`0[],System.Double})">
            <summary>
              Initializes a new instance of KMeans algorithm
            </summary>
            
            <param name="k">The number of clusters to divide input data.</param>       
            <param name="distance">The distance function to use. Default is to
            use the <see cref="M:Accord.Math.Distance.SquareEuclidean(System.Double[],System.Double[])"/> distance.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.Randomize(`0[][])">
            <summary>
              Randomizes the clusters inside a dataset.
            </summary>
            
            <param name="data">The data to randomize the algorithm.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.Compute(`0[][],System.Double)">
            <summary>
              Divides the input data into K clusters. 
            </summary>     
            
            <param name="data">The data where to compute the algorithm.</param>
            <param name="threshold">The relative convergence threshold
            for the algorithm. Default is 1e-5.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.Compute(`0[][],System.Double@,System.Double)">
            <summary>
              Divides the input data into K clusters. 
            </summary>  
            
            <param name="data">The data where to compute the algorithm.</param>
            <param name="threshold">The relative convergence threshold
            for the algorithm. Default is 1e-5.</param>
            
            <param name="error">
              The average distance metric from the
              data points to the clusters' centroids.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.Nearest(`0[])">
            <summary>
              Returns the closest cluster to an input vector.
            </summary>
            
            <param name="point">The input vector.</param>
            <returns>
              The index of the nearest cluster
              to the given data point. </returns>
              
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.Nearest(`0[][])">
            <summary>
              Returns the closest clusters to an input vector array.
            </summary>
            
            <param name="points">The input vector array.</param>
            
            <returns>
              An array containing the index of the nearest cluster
              to the corresponding point in the input array.</returns>
              
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.Error(`0[][])">
            <summary>
              Calculates the average square distance from the data points
              to the clusters' centroids.
            </summary>
            
            <remarks>
              The average distance from centroids can be used as a measure
              of the "goodness" of the clusterization. The more the data
              are aggregated around the centroids, the less the average
              distance.
            </remarks>
            
            <returns>
              The average square distance from the data points to the
              clusters' centroids.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.Error(`0[][],System.Int32[])">
            <summary>
              Calculates the average square distance from the data points
              to the clusters' centroids.
            </summary>
            
            <remarks>
              The average distance from centroids can be used as a measure
              of the "goodness" of the clusterization. The more the data
              are aggregated around the centroids, the less the average
              distance.
            </remarks>
            
            <returns>
              The average square distance from the data points to the
              clusters' centroids.
            </returns>
            
        </member>
        <member name="M:Accord.MachineLearning.KModes`1.converged(`0[][],`0[][],System.Double)">
            <summary>
              Determines if the algorithm has converged by comparing the
              centroids between two consecutive iterations.
            </summary>
            
            <param name="centroids">The previous centroids.</param>
            <param name="newCentroids">The new centroids.</param>
            <param name="threshold">A convergence threshold.</param>
            
            <returns>Returns <see langword="true"/> if all centroids had a percentage change
               less than <see param="threshold"/>. Returns <see langword="false"/> otherwise.</returns>
               
        </member>
        <member name="P:Accord.MachineLearning.KModes`1.Clusters">
            <summary>
              Gets the clusters found by K-modes.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KModes`1.K">
            <summary>
              Gets the number of clusters.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KModes`1.Dimension">
            <summary>
              Gets the dimensionality of the data space.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KModes`1.Distance">
            <summary>
              Gets or sets the distance function used
              as a distance metric between data points.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KModesCluster`1">
            <summary>
              K-modes' Cluster
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KModesCluster`1.Index">
            <summary>
              Gets the label for this cluster.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KModesCluster`1.Mode">
            <summary>
              Gets the cluster's centroid.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KModesCluster`1.Proportion">
            <summary>
              Gets the proportion of samples in the cluster.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KModesClusterCollection`1">
            <summary>
              K-modes Cluster Collection.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.KModesClusterCollection`1.Centroids">
            <summary>
              Gets or sets the clusters' centroids.
            </summary>
            
            <value>The clusters' centroids.</value>
            
        </member>
        <member name="P:Accord.MachineLearning.KModesClusterCollection`1.Proportions">
            <summary>
              Gets the proportion of samples in each cluster.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.KModes">
            <summary>
              K-Modes algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.KModes.#ctor(System.Int32)">
            <summary>
              Initializes a new instance of K-Modes algorithm
            </summary>
            
            <param name="k">The number of clusters to divide input data.</param>    
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod">
            <summary>
              Decision strategies for <see cref="T:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine">
              Multi-class Support Vector Machines</see>.
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod.Voting">
            <summary>
              Max-voting method (also known as 1vs1 decision).
            </summary>
            
        </member>
        <member name="F:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod.Elimination">
            <summary>
              Elimination method (also known as DAG decision).
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine">
             <summary>
               One-against-one Multi-class Kernel Support Vector Machine Classifier.
             </summary>
             <remarks>
             <para>
               The Support Vector Machine is by nature a binary classifier. One of the ways
               to extend the original SVM algorithm to multiple classes is to build a one-
               against-one scheme where multiple SVMs specialize to recognize each of the
               available classes. By using a competition scheme, the original multi-class
               classification problem is then reduced to <c>n*(n/2)</c> smaller binary problems.</para>
             <para>
               Currently this class supports only Kernel machines as the underlying classifiers.
               If a Linear Support Vector Machine is needed, specify a Linear kernel in the
               constructor at the moment of creation. </para>
               
             <para>
               References:
               <list type="bullet">
                 <item><description>
                   <a href="http://courses.media.mit.edu/2006fall/mas622j/Projects/aisen-project/index.html">
                    http://courses.media.mit.edu/2006fall/mas622j/Projects/aisen-project/index.html </a></description></item>
                 <item><description>
                   <a href="http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html">
                    http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html </a></description></item>
                 </list></para>
                 
             </remarks>
            
             <example>
               <code>
               // Sample data
               //   The following is simple auto association function
               //   where each input correspond to its own class. This
               //   problem should be easily solved by a Linear kernel.
            
               // Sample input data
               double[][] inputs =
               {
                   new double[] { 0 },
                   new double[] { 3 },
                   new double[] { 1 },
                   new double[] { 2 },
               };
               
               // Output for each of the inputs
               int[] outputs = { 0, 3, 1, 2 };
               
               
               // Create a new Linear kernel
               IKernel kernel = new Linear();
               
               // Create a new Multi-class Support Vector Machine with one input,
               //  using the linear kernel and for four disjoint classes.
               var machine = new MulticlassSupportVectorMachine(1, kernel, 4);
               
               // Create the Multi-class learning algorithm for the machine
               var teacher = new MulticlassSupportVectorLearning(machine, inputs, outputs);
               
               // Configure the learning algorithm to use SMO to train the
               //  underlying SVMs in each of the binary class subproblems.
               teacher.Algorithm = (svm, classInputs, classOutputs, i, j) =&gt;
                   new SequentialMinimalOptimization(svm, classInputs, classOutputs);
               
               // Run the learning algorithm
               double error = teacher.Run();
               </code>
             </example>
            
             <seealso cref="T:Accord.MachineLearning.VectorMachines.Learning.MulticlassSupportVectorLearning"/>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.#ctor(System.Int32,Accord.Statistics.Kernels.IKernel,System.Int32)">
            <summary>
              Constructs a new Multi-class Kernel Support Vector Machine
            </summary>
            
            <param name="kernel">The chosen kernel for the machine.</param>
            <param name="inputs">The number of inputs for the machine.</param>
            <param name="classes">The number of classes in the classification problem.</param>
            <remarks>
              If the number of inputs is zero, this means the machine
              accepts a indefinite number of inputs. This is often the
              case for kernel vector machines using a sequence kernel.
            </remarks>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.#ctor(Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine[][])">
            <summary>
              Constructs a new Multi-class Kernel Support Vector Machine
            </summary>
            
            <param name="machines">
              The machines to be used in each of the pairwise class subproblems.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[])">
             <summary>
               Computes the given input to produce the corresponding output.
             </summary>
             
             <param name="inputs">An input vector.</param>
             
             <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[],System.Double@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <param name="inputs">An input vector.</param>
            <param name="output">The output of the machine. If this is a 
              <see cref="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.IsProbabilistic">probabilistic</see> machine, the
              output is the probability of the positive class. If this is
              a standard machine, the output is the distance to the decision
              hyperplane in feature space.</param>
            
            <returns>The decision label for the given input.</returns>
            
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[],System.Double@,System.Tuple{System.Int32,System.Int32}[]@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <param name="inputs">An input vector.</param>
            <param name="output">The output of the machine. If this is a 
              <see cref="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.IsProbabilistic">probabilistic</see> machine, the
              output is the probability of the positive class. If this is
              a standard machine, the output is the distance to the decision
              hyperplane in feature space.</param>
            <param name="decisionPath">The decision path followed by the Decision
            Directed Acyclic Graph used by the <see cref="F:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod.Elimination">
            elimination</see> method.</param>
            
            <returns>The decision label for the given input.</returns>
            
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[],System.Double[]@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <param name="inputs">An input vector.</param>
            <param name="responses">The model response for each class.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[],Accord.MachineLearning.VectorMachines.MulticlassComputeMethod,System.Double[]@,System.Double@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <param name="inputs">An input vector.</param>
            <param name="method">The <see cref="T:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod">
              multi-class classification method</see> to use.</param>
            <param name="responses">The model response for each class.</param>
            <param name="output">The output of the machine. If this is a 
              <see cref="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.IsProbabilistic">probabilistic</see> machine, the
              output is the probability of the positive class. If this is
              a standard machine, the output is the distance to the decision
              hyperplane in feature space.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[],Accord.MachineLearning.VectorMachines.MulticlassComputeMethod,System.Double[]@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <param name="inputs">An input vector.</param>
            <param name="method">The <see cref="T:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod">
              multi-class classification method</see> to use.</param>
            <param name="responses">The model response for each class.</param>
            
            <returns>The class decision for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[],Accord.MachineLearning.VectorMachines.MulticlassComputeMethod,System.Double@)">
             <summary>
               Computes the given input to produce the corresponding output.
             </summary>
             
             <param name="inputs">An input vector.</param>
             <param name="method">The <see cref="T:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod">
               multi-class classification method</see> to use.</param>
             <param name="output">The output of the machine. If this is a 
               <see cref="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.IsProbabilistic">probabilistic</see> machine, the
               output is the probability of the positive class. If this is
               a standard machine, the output is the distance to the decision
               hyperplane in feature space.</param>
             
             <returns>The class decision for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[],Accord.MachineLearning.VectorMachines.MulticlassComputeMethod)">
             <summary>
               Computes the given input to produce the corresponding output.
             </summary>
             
             <param name="inputs">An input vector.</param>
             <param name="method">The <see cref="T:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod">
               multi-class classification method</see> to use.</param>
             
             <returns>The class decision for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Reset">
            <summary>
              Resets the cache and machine statistics
              so they can be recomputed on next evaluation.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.GetLastKernelEvaluations">
            <summary>
              Gets the total kernel evaluations performed
              in the last call to any of the <see cref="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Compute(System.Double[])"/>
              functions.
            </summary>
            
            <returns>The number of total kernel evaluations.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.computeVoting(System.Double[],System.Int32[]@,System.Double@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <param name="inputs">An input vector.</param>
            <param name="votes">A vector containing the number of votes for each class.</param>
            <param name="output">The output of the machine. If this is a 
              <see cref="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.IsProbabilistic">probabilistic</see> machine, the
              output is the probability of the positive class. If this is
              a standard machine, the output is the distance to the decision
              hyperplane in feature space.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.computeElimination(System.Double[],System.Double[]@,System.Double@,System.Tuple{System.Int32,System.Int32}[])">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <remarks>
              This method computes the decision for a one-against-one multiclass
              support vector machine using the Directed Acyclic Graph method by
              Platt, Cristianini and Shawe-Taylor. Details are given on the 
              original paper "Large Margin DAGs for Multiclass Classification", 2000.
            </remarks>
            
            <param name="inputs">An input vector.</param>
            <param name="responses">The model response for each class.</param>
            <param name="output">The output of the machine. If this is a 
              <see cref="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.IsProbabilistic">probabilistic</see> machine, the
              output is the probability of the positive class. If this is
              a standard machine, the output is the distance to the decision
              hyperplane in feature space.</param>
            <param name="decisionPath">The decision path followed by the Decision
            Directed Acyclic Graph used by the <see cref="F:Accord.MachineLearning.VectorMachines.MulticlassComputeMethod.Elimination">
            elimination</see> method.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.computeSequential(System.Int32,System.Int32,System.Double[],System.Double@)">
            <summary>
              Compute SVM output with support vector sharing.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.computeParallel(System.Int32,System.Int32,System.Double[],System.Double@)">
            <summary>
              Compute SVM output with support vector sharing.
            </summary>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Save(System.IO.Stream)">
            <summary>
              Saves the machine to a stream.
            </summary>
            
            <param name="stream">The stream to which the machine is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Save(System.String)">
            <summary>
              Saves the machine to a file.
            </summary>
            
            <param name="path">The path to the file to which the machine is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Load(System.IO.Stream)">
            <summary>
              Loads a machine from a stream.
            </summary>
            
            <param name="stream">The stream from which the machine is to be deserialized.</param>
            
            <returns>The deserialized machine.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Load(System.String)">
            <summary>
              Loads a machine from a file.
            </summary>
            
            <param name="path">The path to the file from which the machine is to be deserialized.</param>
            
            <returns>The deserialized machine.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Item(System.Int32,System.Int32)">
            <summary>
              Gets the classifier for <paramref name="class1"/> against <paramref name="class2"/>.
            </summary>
            
            <remarks>
              If the index of <paramref name="class1"/> is greater than <paramref name="class2"/>,
              the classifier for the <paramref name="class2"/> against <paramref name="class1"/>
              will be returned instead. If both indices are equal, null will be
              returned instead.
            </remarks>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.MachinesCount">
            <summary>
              Gets the total number of machines
              in this multi-class classifier.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.SupportVectorCount">
            <summary>
              Gets the total number of support vectors
              in the entire multi-class machine.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.SupportVectorUniqueCount">
            <summary>
              Gets the number of unique support 
              vectors in the multi-class machine.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Classes">
            <summary>
              Gets the number of classes.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Inputs">
            <summary>
              Gets the number of inputs of the machines.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.IsProbabilistic">
            <summary>
              Gets a value indicating whether this machine produces probabilistic outputs.
            </summary>
            
            <value>
              <c>true</c> if this machine produces probabilistic outputs; otherwise, <c>false</c>.
            </value>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MulticlassSupportVectorMachine.Machines">
            <summary>
              Gets the subproblems classifiers.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine">
             <summary>
               One-against-all Multi-label Kernel Support Vector Machine Classifier.
             </summary>
             
             <remarks>
             <para>
               The Support Vector Machine is by nature a binary classifier. Multiple label
               problems are problems in which an input sample is allowed to belong to one
               or more classes. A way to implement multi-label classes in support vector
               machines is to build a one-against-all decision scheme where multiple SVMs
               are trained to detect each of the available classes. </para>
             <para>
               Currently this class supports only Kernel machines as the underlying classifiers.
               If a Linear Support Vector Machine is needed, specify a Linear kernel in the
               constructor at the moment of creation. </para>
               
             <para>
               References:
               <list type="bullet">
                 <item><description>
                   <a href="http://courses.media.mit.edu/2006fall/mas622j/Projects/aisen-project/index.html">
                    http://courses.media.mit.edu/2006fall/mas622j/Projects/aisen-project/index.html </a></description></item>
                 <item><description>
                   <a href="http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html">
                    http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html </a></description></item>
                 </list></para>
                 
             </remarks>
            
             <example>
               <code>
               // Sample data
               //   The following is simple auto association function
               //   where each input correspond to its own class. This
               //   problem should be easily solved by a Linear kernel.
            
               // Sample input data
               double[][] inputs =
               {
                   new double[] { 0 },
                   new double[] { 3 },
                   new double[] { 1 },
                   new double[] { 2 },
               };
               
               // Outputs for each of the inputs
               int[][] outputs =
               {
                   new[] { -1,  1, -1 },
                   new[] { -1, -1,  1 },
                   new[] {  1,  1, -1 },
                   new[] { -1, -1, -1 },
               };
               
               
               // Create a new Linear kernel
               IKernel kernel = new Linear();
               
               // Create a new Multi-class Support Vector Machine with one input,
               //  using the linear kernel and for four disjoint classes.
               var machine = new MultilabelSupportVectorMachine(1, kernel, 3);
               
               // Create the Multi-label learning algorithm for the machine
               var teacher = new MultilabelSupportVectorLearning(machine, inputs, outputs);
               
               // Configure the learning algorithm to use SMO to train the
               //  underlying SVMs in each of the binary class subproblems.
               teacher.Algorithm = (svm, classInputs, classOutputs, i, j) =&gt;
                   new SequentialMinimalOptimization(svm, classInputs, classOutputs);
               
               // Run the learning algorithm
               double error = teacher.Run();
               </code>
             </example>
            
             <seealso cref="T:Accord.MachineLearning.VectorMachines.Learning.MultilabelSupportVectorLearning"/>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.#ctor(System.Int32,Accord.Statistics.Kernels.IKernel,System.Int32)">
            <summary>
              Constructs a new Multi-label Kernel Support Vector Machine
            </summary>
            
            <param name="kernel">The chosen kernel for the machine.</param>
            <param name="inputs">The number of inputs for the machine.</param>
            <param name="classes">The number of classes in the classification problem.</param>
            
            <remarks>
              If the number of inputs is zero, this means the machine
              accepts a indefinite number of inputs. This is often the
              case for kernel vector machines using a sequence kernel.
            </remarks>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.#ctor(Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine[])">
            <summary>
              Constructs a new Multi-label Kernel Support Vector Machine
            </summary>
            
            <param name="machines">
              The machines to be used for each class.
            </param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Compute(System.Double[],System.Double[]@)">
            <summary>
              Computes the given input to produce the corresponding outputs.
            </summary>
            
            <param name="inputs">An input vector.</param>
            <param name="responses">The model response for each class.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Compute(System.Double[])">
            <summary>
              Computes the given input to produce the corresponding outputs.
            </summary>
            
            <param name="inputs">An input vector.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Save(System.IO.Stream)">
            <summary>
              Saves the machine to a stream.
            </summary>
            
            <param name="stream">The stream to which the machine is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Save(System.String)">
            <summary>
              Saves the machine to a file.
            </summary>
            
            <param name="path">The path to the file to which the machine is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Load(System.IO.Stream)">
            <summary>
              Loads a machine from a stream.
            </summary>
            
            <param name="stream">The stream from which the machine is to be deserialized.</param>
            
            <returns>The deserialized machine.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Load(System.String)">
            <summary>
              Loads a machine from a file.
            </summary>
            
            <param name="path">The path to the file from which the machine is to be deserialized.</param>
            
            <returns>The deserialized machine.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Accord#MachineLearning#VectorMachines#ISupportVectorMachine#Compute(System.Double[],System.Double@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <param name="inputs">An input vector.</param>
            <param name="output">The output for the given input.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Classes">
            <summary>
              Gets the number of classes.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Inputs">
            <summary>
              Gets the number of inputs of the machines.
            </summary>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.IsProbabilistic">
            <summary>
              Gets a value indicating whether this machine produces probabilistic outputs.
            </summary>
            
            <value>
              <c>true</c> if this machine produces probabilistic outputs; otherwise, <c>false</c>.
            </value>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.MultilabelSupportVectorMachine.Machines">
            <summary>
              Gets the subproblems classifiers.
            </summary>
            
        </member>
        <member name="T:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine">
            <summary>
             Sparse Kernel Support Vector Machine (kSVM)
            </summary>
            <remarks>
            <para>
              The original optimal hyperplane algorithm (SVM) proposed by Vladimir Vapnik in 1963 was a
              linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vapnik suggested
              a way to create non-linear classifiers by applying the kernel trick (originally proposed
              by Aizerman et al.) to maximum-margin hyperplanes. The resulting algorithm is formally
              similar, except that every dot product is replaced by a non-linear kernel function.</para>
            <para>
              This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space.
              The transformation may be non-linear and the transformed space high dimensional; thus though
              the classifier is a hyperplane in the high-dimensional feature space, it may be non-linear in
              the original input space.</para> 
            <para>
              References:
              <list type="bullet">
                <item><description><a href="http://en.wikipedia.org/wiki/Support_vector_machine">
                  http://en.wikipedia.org/wiki/Support_vector_machine</a></description></item>
                <item><description><a href="http://www.kernel-machines.org/">
                  http://www.kernel-machines.org/</a></description></item>
              </list></para>  
            </remarks>
            
            <example>
              <code>
              // Example XOR problem
              double[][] inputs =
              {
                  new double[] { 0, 0 }, // 0 xor 0: 1 (label +1)
                  new double[] { 0, 1 }, // 0 xor 1: 0 (label -1)
                  new double[] { 1, 0 }, // 1 xor 0: 0 (label -1)
                  new double[] { 1, 1 }  // 1 xor 1: 1 (label +1)
              };
              
              // Dichotomy SVM outputs should be given as [-1;+1]
              int[] labels =
              {
                  // 1,  0,  0, 1
                     1, -1, -1, 1
              };
              
              // Create a Kernel Support Vector Machine for the given inputs
              KernelSupportVectorMachine machine = new KernelSupportVectorMachine(new Gaussian(0.1), inputs[0].Length);
              
              // Instantiate a new learning algorithm for SVMs
              SequentialMinimalOptimization smo = new SequentialMinimalOptimization(machine, inputs, labels);
              
              // Set up the learning algorithm
              smo.Complexity = 1.0;
              
              // Run the learning algorithm
              double error = smo.Run();
              
              // Compute the decision output for one of the input vectors
              int decision = System.Math.Sign(svm.Compute(inputs[0]));
              </code>
            </example>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine.#ctor(Accord.Statistics.Kernels.IKernel,System.Int32)">
            <summary>
              Creates a new Kernel Support Vector Machine.
            </summary>
            
            <param name="kernel">The chosen kernel for the machine.</param>
            <param name="inputs">The number of inputs for the machine.</param>
            
            <remarks>
              If the number of inputs is zero, this means the machine
              accepts a indefinite number of inputs. This is often the
              case for kernel vector machines using a sequence kernel.
            </remarks>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine.Compute(System.Double[],System.Double@)">
            <summary>
              Computes the given input to produce the corresponding output.
            </summary>
            
            <remarks>
              For a binary decision problem, the decision for the negative
              or positive class is typically computed by taking the sign of
              the machine's output.
            </remarks>
            
            <param name="inputs">An input vector.</param>
            <param name="output">The output of the machine. If this is a 
              <see cref="P:Accord.MachineLearning.VectorMachines.SupportVectorMachine.IsProbabilistic">probabilistic
              </see> machine, the output is the probability of the positive
              class. If this is a standard machine, the output is the distance
              to the decision hyperplane in feature space.</param>
            
            <returns>The decision label for the given input.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine.Save(System.IO.Stream)">
            <summary>
              Saves the machine to a stream.
            </summary>
            
            <param name="stream">The stream to which the machine is to be serialized.</param>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine.Load(System.IO.Stream)">
            <summary>
              Loads a machine from a stream.
            </summary>
            
            <param name="stream">The stream from which the machine is to be deserialized.</param>
            
            <returns>The deserialized machine.</returns>
            
        </member>
        <member name="M:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine.Load(System.String)">
            <summary>
              Loads a machine from a file.
            </summary>
            
            <param name="path">The path to the file from which the machine is to be deserialized.</param>
            
            <returns>The deserialized machine.</returns>
            
        </member>
        <member name="P:Accord.MachineLearning.VectorMachines.KernelSupportVectorMachine.Kernel">
            <summary>
              Gets or sets the kernel used by this machine.
            </summary>
            
        </member>
    </members>
</doc>
